{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q learning\n",
    "We have already seen [Q learning](../../1-grid-world/7-q-learning/q_learning_agent.ipynb) in the previous examples.\n",
    "Q learning is a special algorithm that provides an off policy method for Temporal Difference style control.\n",
    "Deep Q learning is just an extension of Q learning that uses neural networks, i.e. a *non-linear* function as state-action value function approximator.\n",
    "This is done to scale up to decision making in really large domains of huge state spaces.\n",
    "\n",
    "Deep Q learning in particular has proven to achieve great results in games, as we will see in this example too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of deep Q learning\n",
    "Deep Q learning inherits all the characteristics of [Q learning](../7-q-learning/q_learning_agent.ipynb), except that it allows us to solve problems with continuous or very large state and action spaces.\n",
    "\n",
    "##### Neural Network as value function approximator\n",
    "This can be any neural network architecture that we see fit.\n",
    "In this example, we are using a very simple Dense Neural Network architecture.\n",
    "Dense Neural Networks are simply a *stack of matrices with activation functions in between*.\n",
    "\n",
    "##### Replay memory\n",
    "Each tuple of $(s_t, a_t, r, s_{t+1})$ is recorded in a replay memory.\n",
    "As a replay memory we use a FIFO dequeue of constant size.\n",
    "\n",
    "##### Batch training\n",
    "Each step taken by the agent, we feed a *random* sample of tuples from the replay memory to the neural network in order to do *batch training*.\n",
    "\n",
    "##### Main model and Target model\n",
    "We keep two neural network models in our implementation.\n",
    "The reason we keep a second one, called the target model, is to provide some **stability** while training.\n",
    " - with each step taken we update only the first main model.\n",
    " - after some time interval (in this case, every episode), we update the second target model to be the same with the main model.\n",
    "\n",
    "##### Continuous or very large state and action spaces\n",
    "Using a neural network as a value function approximator has its benefits.\n",
    "It allows us to solve problems with continuous state and action spaces.\n",
    "Such problems would be almost impossible to solve with normal Q learning, as they are very resource hungry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialization\n",
    "For the Q learning aspect we keep track of the following:\n",
    " - In order to showcase how robust off policy algorithms like Q learning are, we are going to keep the epsilon and learning rate constant\n",
    "   - `self.learning_rate` is set to $0.001$\n",
    "   - `self.epsilon` is initially set to $1.0$ and decays each step taken via the variable `self.epsilon_decay = 0.999` until a minimum of `self.epsilon_min = 0.01` is reached\n",
    " - `self.discount_factor` is set to $0.99$\n",
    " - We keep track of a **replay memory** of size $2000$ via the following `self.memory = deque(maxlen=2000)`\n",
    "\n",
    "For the neural network aspect we keep track of the following:\n",
    " - we keep track of a model (neural network) in `self.model = self.build_model()`\n",
    " - we also keep track of a target model `self.target_model = self.build_model()`.\n",
    " After some time interval we update the target model to be the same with the main model to provide some stability.\n",
    " - we only train after there are at least $1000$ entries in the replay memory.\n",
    " We specify this at `self.train_start = 1000`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "# DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        # contains <s,a,r,s'> tuples\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.next_states_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_dqn.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep Q learning\n",
    "\n",
    "The update rule for Q values in Deep Q learning is the following:\n",
    "\n",
    "$\\hat{Q}^\\pi(s_t, a_t) \\gets^{train} R_t+ \\gamma max_{a’} \\hat{Q}^\\pi(s_t ,a’)$\n",
    " - $\\hat{Q}^\\pi(s_t, a_t)$ - *predicted* Q value of current state-action pair following the policy $\\pi$\n",
    " - $\\gets^{train}$ - this means train the neural network accordingly, instead of *assign* the value\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " - $max_{a’} \\hat{Q}^\\pi(s_t ,a’)$ - maximization operator over the **predicted** Q values of all possible actions in the current state\n",
    "\n",
    "First things first, the update formula is very similar to the update rule we saw in Q learning, although with the following differences:\n",
    " - instead of $Q^\\pi$ we now deal with $\\hat{Q}^\\pi$, which is an approximation, the output of a neural network.\n",
    " - after calculating the result, we do not *assign* the value to $\\hat{Q}^\\pi$, but rather *train* the neural network with *gradient descent* in order to update the weights with the latest state-action value.\n",
    "\n",
    "Moreover, this seems like a simplified version of the update rule we saw for Q learning.\n",
    "Notice that the right hand side of the formula is simply a **bootstrapped return**.\n",
    "We use this value to update the network and we do not take into account the temporal difference between Q values.\n",
    "\n",
    "The reason behind this is that in Deep Q learning, the presence of a neural network will mimic the temporal difference formula we saw on Q learning with a *learning rate*.\n",
    "In a neural network we update the weights via *backpropagation* in *gradient descent*.\n",
    "We also specify a **learning rate** in the process.\n",
    "That is why we do not need the explicit temporal difference aspect of Q learning anymore, since a similar implicit process is provided by backpropagation when we train the neural network with new data.\n",
    "\n",
    "Moreover, keep in mind these two characteristics of training that we also mentioned above:\n",
    " - replay memory\n",
    " - batch training\n",
    " - stability with target networks\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # get s (state) as input from mini_batch\n",
    "        # initialize with shape batch_size x state_size\n",
    "        curr_states = np.zeros((batch_size, self.state_size))\n",
    "        # get s' (next state) as input from mini_batch\n",
    "        # initialize with shape batch_size x state_size\n",
    "        next_states = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # get s (state) as input from mini_batch\n",
    "            curr_states[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            # get s' (next state) as input from mini_batch\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(curr_states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # selection of best action is from *target* model\n",
    "                # evaluation is also from target model\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(curr_states, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The model: neural network\n",
    "Neural networks are built one layer after the other.\n",
    "Our model is a dense neural network, i.e. a neural network comprised of only dense layers.\n",
    "Dense layers are simply a *set of matrices* with an *activation function* in the end."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(16, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Setting target network for stability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def next_states_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Main loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.next_states_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_dqn2.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_dqn2.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "<h3 style=\"text-align:center\">Results</h3>\n",
    "<img src=\"./save_graph/Cartpole_DQN.png\" alt=\"Cartpole_DQN.png\" width=\"70%\" />\n",
    "\n",
    "From the graph above we can see the following:\n",
    " - The agent starts learning after about 80 episodes.\n",
    " This happens because in the first 80 episodes, it is exploring and growing its experience replay.\n",
    " Training starts only after the experience replay reaches a size of 1000 tuples, which happens after almost 80 episodes.\n",
    " - DQN is clearly showing maximization bias, because of the big difference between the lower bounds and the upper bounds.\n",
    " - The upper bound after 100 episodes is 500. This happens because of maximization bias and experience replay.\n",
    " - The lower bound after 100 episodes is continuously getting better, indicating that if left enough time to train, the algorithm will converge to always hitting the 500 points.\n",
    "\n",
    "After 100 episodes, the epsilon is nearly 0.01, meaning there is nearly no exploration going on.\n",
    "The reason why the agent is able to learn even after 100 episodes is exactly because of experience replay.\n",
    "Training from the experience replay is done in random batches of size 64.\n",
    "After 100 episodes, the experience replay is full of tuples with good choices in actions and after that only better and better tuples are inserted.\n",
    "So after 100 episodes, only good experiences are reinforced to the agent, allowing for surges in scores and always hitting the 500 points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}