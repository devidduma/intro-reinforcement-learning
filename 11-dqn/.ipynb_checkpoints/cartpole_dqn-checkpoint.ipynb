{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Q learning\n",
    "We have already seen [Q learning](../../1-grid-world/7-q-learning/q_learning_agent.ipynb) in the previous examples.\n",
    "Q learning is a special algorithm that provides an off policy method for Temporal Difference style control.\n",
    "Deep Q learning is just an extension of Q learning that uses neural networks as state-action value function approximators.\n",
    "\n",
    "Deep Q learning has the following update rule:  $\\hat{Q}^\\pi(s_t, a_t) \\gets^{train} \\hat{Q}^\\pi(s_t, a_t) + \\alpha[R_t+ \\gamma max_{a’} \\hat{Q}^\\pi(s_t ,a’) - \\hat{Q}^\\pi(s_t,a_t)]$.\n",
    "It is very similar to the update rule we saw in Q learning, though with the following differences:\n",
    " - instead of $Q^\\pi$ we now deal with $\\hat{Q}^\\pi$, which is an approximation, the output of a neural network.\n",
    " - after calculating the result, we do not *assign* the value to $\\hat{Q}^\\pi$, but rather *train* the neural network with *gradient descent* in order to update the weights with the latest state-action value.\n",
    "\n",
    "Deep Q learning in particular has proven to achieve great results in games, as we will see in this example too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Characteristics of Deep Q learning\n",
    "\n",
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
