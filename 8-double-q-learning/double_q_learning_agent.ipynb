{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Double Q learning\n",
    "Double Q learning is an improvement over the standard Q learning algorithm that solves the maximization bias problem from which Q-learning suffers.\n",
    "In double Q learning, we maintain two independent unbiased estimates, Q1 and Q2 and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum.\n",
    "With 0.5 probability we update Q1 and with 0.5 probability we update Q2.\n",
    "\n",
    "The update rules now become the following two formulas:\n",
    " - $Q_1(s_t, a_t) \\gets Q_1(s_t, a_t) + \\alpha(r_t+ \\gamma Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’)) - Q_1(s_t,a_t))$\n",
    " - $Q_2(s_t, a_t) \\gets Q_2(s_t, a_t) + \\alpha(r_t+ \\gamma Q_1(s_{t+1}, argmax_{a’} Q_2(s_t, a’)) - Q_2(s_t,a_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Double Q learning:\n",
    " - Double Q learning inherits all the characteristics of [Q learning](../7-q-learning/q_learning_agent.ipynb), except **maximization bias**.\n",
    " - It is an improvement of Q learning that addresses a solution specifically for maximization bias.\n",
    "\n",
    "##### No maximization bias\n",
    "We maintain two independent unbiased estimates, Q1 and Q2 and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum.\n",
    "Decoupling taking the max and estimating the value of the max can get rid of maximization bias.\n",
    "\n",
    "##### Significantly faster training than normal Q learning\n",
    "Double Q learning can significantly speed up training time by eliminating suboptimal actions more quickly than normal Q learning."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialization\n",
    "For Q learning we keep track of the following:\n",
    " - Instead of having one dictionary of Q values like in Q learning, in double Q learning we keep **two** dictionaries of Q values.\n",
    " Both Q value functions are initially set to $0$.\n",
    " - In order to showcase how robust off policy algorithms like Q learning are, we are going to keep the epsilon and learning rate constant.\n",
    "   - `self.learning_rate` is set to $0.4$.\n",
    "   - `self.epsilon` is set to $0.1$.\n",
    " - `self.discount_factor` is set to $0.9$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from environment import Env\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions):\n",
    "        # actions = [0, 1, 2, 3]\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.4\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.qA_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "        self.qB_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Double Q learning\n",
    "\n",
    "Double Q learning is an off policy algorithm that combines sampling with bootstrapping in an off policy algorithm.\n",
    "\n",
    "Instead of having one dictionary of Q values like in Q learning, in double Q learning we keep **two** dictionaries of Q values.\n",
    "The update rule for Q values in double Q learning is the following:\n",
    "- $Q_1(s_t, a_t) \\gets Q_1(s_t, a_t) + \\alpha(r_t+ \\gamma Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’)) - Q_1(s_t,a_t))$\n",
    "- $Q_2(s_t, a_t) \\gets Q_2(s_t, a_t) + \\alpha(r_t+ \\gamma Q_1(s_{t+1}, argmax_{a’} Q_2(s_t, a’)) - Q_2(s_t,a_t))$\n",
    "\n",
    "##### Action selection step via argmax operator\n",
    "We use one of the networks to select the action leading to the maximum, for example:\n",
    "$argmax_{a’} Q_1(s_t, a’)$\n",
    "\n",
    "##### Q value estimation step\n",
    "We then use the other network to *estimate* the Q function:\n",
    "$Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’))$\n",
    "\n",
    "##### Update step\n",
    "We then update the Q value of the network used in action selection:\n",
    "$Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’))$.\n",
    "With $0.5$ probability we update $Q_1$ and with $0.5$ probability we update $Q_2$.\n",
    "\n",
    "\n",
    "Simple as that.\n",
    " - $Q_x(s_t, a_t)$ - Q value of current state-action pair following the policy $\\pi$\n",
    " - $Q_x(s_{t+1}, argmax_{a’} Q_y(s_t, a’))$ - action selected at $Q_y$ is evaluated at $Q_x$\n",
    " - $\\alpha$ - the **learning rate**.\n",
    " Learning rate can take any value int the range $[0...1]$.\n",
    " Values closer to 0 mean that we put more value to older experiences, whereas values closer to 1 means that we put more value to latest experiences.\n",
    " In our case, the learning rate takes the value $0.4$.\n",
    " - $r_t$ - the reward at time-step $t$.\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " Traditionally used when calculating returns, now it is used when calculating **expectancies of returns**, i.e. state values.\n",
    "\n",
    "The difference $r_t+ \\gamma Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’)) - Q_1(s_t,a_t)$ is commonly referred to as the **TD error**.\n",
    "\n",
    "The sum $r_t+ \\gamma Q_2(s_{t+1}, argmax_{a’} Q_1(s_t, a’)$ is referred to as the **TD target**.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    # update q function with sample <s, a, r, s'>\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # choose which table will be updated randomly\n",
    "        if np.random.rand() < 0.5:\n",
    "            q_table = self.qA_table\n",
    "        else:\n",
    "            q_table = self.qB_table\n",
    "\n",
    "        current_q = q_table[state][action]\n",
    "        # using Bellman Optimality Equation to update q function\n",
    "        QL_Target = reward + self.discount_factor * max(q_table[next_state])\n",
    "        QL_Error = QL_Target - current_q\n",
    "        q_table[state][action] = current_q + self.learning_rate * QL_Error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    # get action for the state according to the q function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function tables\n",
    "            state_action_A = self.qA_table[state]\n",
    "            state_action_B = self.qB_table[state]\n",
    "            state_action_ABsum = [sum(x) for x in zip(state_action_A, state_action_B)]\n",
    "            action = self.arg_max(state_action_ABsum)\n",
    "        return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Main loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    def mainloop(self, env, verbose=False):\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # take action and proceed one step in the environment\n",
    "                action = self.get_action(str(state))\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                # with sample <s,a,r,s'>, agent learns new q function\n",
    "                self.learn(str(state), action, reward, str(next_state))\n",
    "\n",
    "                state = next_state\n",
    "                env.print_value_all(self.qA_table, self.qB_table)\n",
    "\n",
    "                # if episode ends, then break\n",
    "                if done:\n",
    "                    if verbose:\n",
    "                        print(\"episode: \", episode,\n",
    "                              \"\\tepsilon: \", round(self.epsilon, 2),\n",
    "                              \"\\tlearning rate: \", round(self.learning_rate, 2)\n",
    "                              )\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = QLearningAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env, verbose=False)\n",
    "    except:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "Double Q learning does converge to an optimal policy within 60 episodes.\n",
    "\n",
    "Double Q learning, being an off policy algorithm, shows very robust results *even when the learning rates do not satisfy the Robbins-Munro sequence condition*.\n",
    "\n",
    "In the image below you can see the Q values of the agent after 100 episodes.\n",
    "The reason why there are Q values of above 100 up to 200, is that in double Q learning, having two networks, the sum of the Q values of the two networks for each state make up the Q value of that state.\n",
    "That being said, they are double the actual expectancies for each state.\n",
    "\n",
    "This in turn does nothing to our agent.\n",
    " - all the Q values have an upper bound, and in this case twice the maximal reward of 100.\n",
    " - the Q values retain the same **true** ratio with each other, **without** a maximization bias like in Q learning.\n",
    "\n",
    "<h3 style=\"text-align:center\">After 100 episodes</h3>\n",
    "<img src=\"ipynb_results/double_q_learning_100_episodes.png\" alt=\"double_q_learning_100_episodes.png\" width=\"50%\" />\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}