{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Double deep Q networks\n",
    "Double deep Q-networks implement the [double Q-learning](../8-double-q-learning/double_q_learning_agent.ipynb) algorithm with a neural network for Q values approximation.\n",
    "To prevent overestimation and reduce maximization bias, DDQN decouples the action selection from Q value evaluation.\n",
    "In a DDQN there are two neural networks at play, one current network and one older version of that network.\n",
    "The first network is always improving and is used to select the action, meaning that it uses the maximization operator over the Q values to select the best action.\n",
    "The older network is used to evaluate the V value of that action.\n",
    "Finally, every now and then the old network is set equal to the current network, meaning that the old network gets updated to a better state, but less frequently than in simple DQN.\n",
    "DDQN is shown to reduce bias and improve performance on the same set of problems that DQN is used.\n",
    "DDQN can be further improved using an experience replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of double deep Q network\n",
    "Double deep Q network inherits all the characteristics of [double Q learning](../8-double-q-learning/double_q_learning_agent.ipynb), except that it allows us to solve problems with continuous or very large state and action spaces.\n",
    "\n",
    "##### Neural Network as value function approximator\n",
    "This can be any neural network architecture that we see fit.\n",
    "In this example, we are using a very simple Dense Neural Network architecture.\n",
    "Dense Neural Networks are simply a *stack of matrices with activation functions in between*.\n",
    "\n",
    "##### Replay memory\n",
    "Each tuple of $(s_t, a_t, r, s_{t+1})$ is recorded in a replay memory.\n",
    "As a replay memory we use a FIFO dequeue of constant size.\n",
    "\n",
    "##### Batch training\n",
    "Each step taken by the agent, we feed a *random* sample of tuples from the replay memory to the neural network in order to do *batch training*.\n",
    "\n",
    "##### Main model and Target model\n",
    "We keep two neural network models in our implementation.\n",
    "The reason we keep a second one, called the target model, is to provide some **stability** while training.\n",
    " - with each step taken we update only the first main model.\n",
    " - after some time interval (in this case, every episode), we update the second target model to be the same with the main model.\n",
    "\n",
    "##### Continuous or very large state and action spaces\n",
    "Using a neural network as a value function approximator has its benefits.\n",
    "It allows us to solve problems with continuous state and action spaces.\n",
    "Such problems would be almost impossible to solve with normal Q learning, as they are very resource hungry.\n",
    "\n",
    "##### Decoupling action selection and evaluation\n",
    "This is the main difference with [deep Q network](../11-dqn/cartpole_dqn.ipynb).\n",
    "In double deep Q network, like in deep Q network we maintain two neural networks: one main model and one target model.\n",
    "In double deep Q network, selection of best action is done from the current model, whereas the evaluation of that action is done from the target model.\n",
    "This has proven to reduce maximisation bias.\n",
    "```\n",
    "target = self.model.predict(curr_states)\n",
    "target_next = self.model.predict(next_states)\n",
    "target_val = self.target_model.predict(next_states)\n",
    "\n",
    "for i in range(self.batch_size):\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    if done[i]:\n",
    "        target[i][action[i]] = reward[i]\n",
    "    else:\n",
    "        # the key point of Double DQN\n",
    "        # selection of best action is from model\n",
    "        a = np.argmax(target_next[i])\n",
    "        # evaluation is from target model\n",
    "        target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "            target_val[i][a])\n",
    "```\n",
    "By contrast, in deep Q network, selection of best action and evaluation of that action are both done in the target model.\n",
    "```\n",
    "target = self.model.predict(curr_states)\n",
    "target_val = self.target_model.predict(next_states)\n",
    "for i in range(self.batch_size):\n",
    "    # Q Learning: get maximum Q value at s' from target model\n",
    "    if done[i]:\n",
    "        target[i][action[i]] = reward[i]\n",
    "    else:\n",
    "        # selection of best action is from *target* model\n",
    "        # evaluation is also from target model\n",
    "        target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "            np.amax(target_val[i]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n",
    "For the double Q learning aspect we keep track of the following:\n",
    " - In order to showcase how robust off policy algorithms like Q learning are, we are going to keep the epsilon and learning rate constant\n",
    "   - `self.learning_rate` is set to $0.001$\n",
    "   - `self.epsilon` is initially set to $1.0$ and decays each step taken via the variable `self.epsilon_decay = 0.999` until a minimum of `self.epsilon_min = 0.01` is reached\n",
    " - `self.discount_factor` is set to $0.99$\n",
    " - We keep track of a **replay memory** of size $2000$ via the following `self.memory = deque(maxlen=2000)`\n",
    "\n",
    "For the neural network aspect we keep track of the following:\n",
    " - we keep track of a model (neural network) in `self.model = self.build_model()`\n",
    " - we also keep track of a target model `self.target_model = self.build_model()`.\n",
    " After some time interval we update the target model to be the same with the main model to provide some stability.\n",
    " Also note that here, the target network is used to only *evaluate* the selected actions.\n",
    " - we only train after there are at least $1000$ entries in the replay memory.\n",
    " We specify this at `self.train_start = 1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "# Double DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # these is hyper parameters for the Double DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.learning_rate_decay = 0.0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        # contains <s,a,r,s'> tuples\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.next_states_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model.load_weights(\"./save_model/cartpole_ddqn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Double deep Q network\n",
    "\n",
    "The update rule for Q values in double deep Q network follows the same logic as in [double Q network](../8-double-q-learning/double_q_learning_agent.ipynb):\n",
    "\n",
    "$\\hat{Q}^\\pi(s_t, a_t) \\gets^{train} R_t+ \\gamma \\hat{Q}^\\pi_{target}(s_{t+1}, argmax_{a’} \\hat{Q}_{current}^\\pi(s_t ,a’))$\n",
    " - $\\hat{Q}^\\pi(s_t, a_t)$ - *predicted* Q value of current state-action pair following the policy $\\pi$\n",
    " - $\\gets^{train}$ - this means train the neural network accordingly, instead of *assign* the value\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " - $argmax_{a’} \\hat{Q}_{current}^\\pi(s_t ,a’)$ - this is the *action selection step*, done by using the current network model.\n",
    " argmax operator over the **predicted** Q values of all possible actions in the current state.\n",
    " - $\\hat{Q}^\\pi_{target}(...)$ - this is the *action evaluation* step.\n",
    "\n",
    "First things first, the update formula is very similar to the update rule we saw in double Q learning, although with the following differences:\n",
    " - instead of $Q^\\pi$ we now deal with $\\hat{Q}^\\pi$, which is an approximation, the output of a neural network.\n",
    " - after calculating the result, we do not *assign* the value to $\\hat{Q}^\\pi$, but rather *train* the neural network with *gradient descent* in order to update the weights with the latest state-action value.\n",
    "\n",
    "Moreover, this seems like a simplified version of the update rule we saw for double Q learning.\n",
    "Notice that the right hand side of the formula is simply a **bootstrapped return**.\n",
    "We use this value to update the network and we do not take into account the temporal difference between Q values.\n",
    "\n",
    "The reason behind this is that in double deep Q network, the presence of a neural network will mimic the temporal difference formula we saw on Q learning with a *learning rate*.\n",
    "In a neural network we update the weights via *backpropagation* in *gradient descent*.\n",
    "We also specify a **learning rate** in the process.\n",
    "That is why we do not need the explicit temporal difference aspect of Q learning anymore, since a similar implicit process is provided by backpropagation when we train the neural network with new data.\n",
    "\n",
    "Moreover, keep in mind these two characteristics of training that we also mentioned above:\n",
    " - replay memory\n",
    " - batch training\n",
    " - stability with target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DoubleDQNAgent):\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        # get s (state) as input from mini_batch\n",
    "        # initialize with shape batch_size x state_size\n",
    "        curr_states = np.zeros((batch_size, self.state_size))\n",
    "        # get s' (next state) as input from mini_batch\n",
    "        # initialize with shape batch_size x state_size\n",
    "        next_states = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # get s (state) as input from mini_batch\n",
    "            curr_states[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            # get s' (next state) as input from mini_batch\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(curr_states)\n",
    "        target_next = self.model.predict(next_states)\n",
    "        target_val = self.target_model.predict(next_states)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of best action is from model\n",
    "                a = np.argmax(target_next[i])\n",
    "                # evaluation is from target model\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    target_val[i][a])\n",
    "\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(curr_states, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### The model: neural network\n",
    "Neural networks are built one layer after the other.\n",
    "Our model is a dense neural network, i.e. a neural network comprised of only dense layers.\n",
    "Dense layers are simply a *set of matrices* with an *activation function* in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DoubleDQNAgent):\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(24, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting target network for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DoubleDQNAgent):\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def next_states_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DoubleDQNAgent):\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DoubleDQNAgent):\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, you can play until 500 time step\n",
    "    env = gym.make('CartPole-v1')\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -100\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            agent.train_model()\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.next_states_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 100\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/cartpole_ddqn2.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    sys.exit()\n",
    "\n",
    "        # save the model\n",
    "        if e % 50 == 0:\n",
    "            agent.model.save_weights(\"./save_model/cartpole_ddqn2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 style=\"text-align:center\">Results</h3>\n",
    "<img src=\"./save_graph/cartpole_ddqn.png\" alt=\"cartpole_ddqn.png\" width=\"70%\" />\n",
    "\n",
    "We can see that the graph of DDQN does not differ too much from the graph shown in [DQN](../11-dqn/cartpole_dqn.ipynb).\n",
    "There is clearly less maximization bias, although the fact that we start training after 80 episodes means that some differences between the upper and lower bound of scores in the beginning is normal.\n",
    "\n",
    "We can see that the lower bounds do converge almost to the same rate as the lower bounds of DQN.\n",
    "Hitting the 500 mark is also less frequent, since in DDQN there is no maximization bias.\n",
    "\n",
    "Nevertheless, we can clearly see that maximization bias is not counterproductive in the case of CartPole-v1 environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}