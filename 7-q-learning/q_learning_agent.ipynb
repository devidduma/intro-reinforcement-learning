{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q learning\n",
    "Q-learning is a special algorithm that provides an off policy method for Temporal Difference style control.\n",
    "First of all, it analyzes the SARSA update rule and modifies it in order to bootstrap the Q value at the next state before choosing and executing an action.\n",
    "Thus, the update rule becomes: $Q_\\pi(s_t, a_t) \\gets Q_\\pi(s_t, a_t) + \\alpha[R_t+ \\gamma max_{a’} Q_\\pi(s_t ,a’) - Q_\\pi(s_t,a_t)]$.\n",
    "This introduces what is called a maximization bias, which can be good or bad depending on the problem, although it is usually a drawback instead of an advantage.\n",
    "In other words, it introduces some bias in the agent that some actions are better than others, even though the actual real Q values might be different.\n",
    "Nevertheless, Q-learning has proven to achieve great results in games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Q learning:\n",
    "\n",
    "##### Model free\n",
    "Q learning is model free, i.e. it does not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### Finite and discrete state and action spaces\n",
    "In order for Q learning to work, the environment has to have a finite state and action space, because it saves state-action values in a dictionary internally.\n",
    "The aforementioned is only possible if the state and action spaces are finite and discrete.\n",
    "\n",
    "##### Off policy\n",
    " - On policy methods attempt to evaluate or improve the policy that is used to make decisions.\n",
    " - Off policy methods evaluate or improve a policy different from that used to generate the data.\n",
    " Something very positive for off policy methods is that they can figure out the optimal policy **regardless** of the agent’s actions and motivation.\n",
    "\n",
    "Q learning in particular is an off policy algorithm.\n",
    "\n",
    "##### Q values: state-action pair values\n",
    "Q learning evaluates and updates Q values, i.e. state-action pair values instead of state values, i.e. V values.\n",
    "This basically means that it does not bind the expected future sum of rewards to a state, but rather binds the expectancies to state-action pairs.\n",
    "\n",
    "##### Convergence\n",
    "Q learning is guaranteed to converge to a *local optimum* in case of infinite visits to state-action pairs.\n",
    "\n",
    "##### Bootstrapping & Sample based\n",
    "Q learning combines bootstrapping with sampling, just like SARSA also did.\n",
    " - The bootstrapping aspect is common with previous dynamic programming methods.\n",
    " The update formula consists in a Bellman backup over just one transition. It is executed every transition and the state-action value for the current state-action pair is bootstrapped from the old estimate of the sampled next state - next action pair.\n",
    " The reason we are able to backup over just one transition is because we leverage the **Markovian assumption** of the domain.\n",
    " - Sampling is common with Monte Carlo methods in order to allow for a model free algorithm.\n",
    "\n",
    "##### Decoupling the Q function estimator from the actual policy\n",
    "Since Q learning is off policy, we introduce the notion of a Q value estimator different from the actual policy.\n",
    "This estimator is essentially a maximization operator over the possible actions of the next state.\n",
    "\n",
    "This allows us to bootstrap the Q value of the next state **without** actually executing an action in the next state.\n",
    "\n",
    "##### Maximization bias\n",
    "We introduced a maximization operator as estimator of the Q value over the possible actions of the next state.\n",
    "A downside to this is that estimators with unusually large values at time $t$ are chosen more frequently, and this leads to an exaggeration of the true value of the next state.\n",
    "\n",
    "In the end, our state value estimate is **at least as large** as the true value of state $s$, so we are systematically overestimating the value of the state in the presence of finite samples.\n",
    "\n",
    "##### Finite & Infinite Horizon\n",
    "Q learning can be used in both finite or infinite horizon settings, i.e. it works with both episodic or non-episodic domains.\n",
    "Infinite horizon settings are possible because SARSA update rules for the Q value function happen each step, not after the end of an episode like in Monte Carlo.\n",
    "\n",
    "##### Lower variance and lower data efficiency than Monte Carlo\n",
    "Q learning has lower variance and lower data efficiency when compared to Monte Carlo methods.\n",
    "This is a property inherited from Temporal Difference learning.\n",
    "For more information, have a look at [Temporal Difference learning](../4-temporal-difference/td_agent.ipynb).\n",
    "\n",
    "##### Lower data efficiency when using Q values than using V values\n",
    "Agents that work with state-action pair values usually have lower data efficiency than their counterparts working with state values while exploring.\n",
    " - more memory is used to represent state-action pair values than state values\n",
    " - more data is needed to train the agent, i.e. the agent needs to spend more time interacting with the environment to learn a good policy while exploring\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Epsilon greedy policies determine how often will the agent explore and how often will the agent exploit.\n",
    "\n",
    "Furthermore, we want the epsilon greedy policy to be **greedy in the limit of exploration (GLIE)**.\n",
    " - all state-action pairs are visited an infinite number of times\n",
    " - $\\epsilon_{t} → 0$ as $ t → 0 $, i.e. the policy is greedy in the limit and converges to 0\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon \\gets 1 / ( c_{\\epsilon} \\times f_{\\epsilon})$, where $ c_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ f_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value in the range $[0...1]$.\n",
    " - setting it to $1$ means that we put as much value to future states as the current state.\n",
    " - setting it to $0$ means that we do not value future states at all, only the current state.\n",
    "\n",
    "##### Learning rate\n",
    "The learning rate *usually* takes any value in the range $[0...1]$.\n",
    " - setting a value bigger than $1$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    " - values closer to $0$ gives a higher weight to older data.\n",
    " - values closer to $1$ gives almost the same weight to old and new data.\n",
    "\n",
    "##### Theorem: Robbins-Munro sequence for Learning rate\n",
    "Because Q learning is off-policy, it will learn an locally optimal policy independent of the agent’s actions.\n",
    "So Q learning **empirically** figures out the optimal policy regardless of the agent’s motivation.\n",
    "\n",
    "Nevertheless, mathematically speaking there is **no** guaranteed convergence to such an optimal policy.\n",
    "This means that even though Q learning can be more robust than SARSA in finding optimal policies empirically, bad cases can happen, even more so when it suffers from maximisation bias.\n",
    "\n",
    "To **guarantee** convergence we again introduce the following conditions.\n",
    "\n",
    "Finite-state and finite-action MDP's converges to the optimal action-value, i.e. Q(s, a) → q(s, a), if the following two conditions hold:\n",
    " 1. The sequence of policies $\\pi$ is GLIE\n",
    " 2. The step-sizes $\\alpha_t$ satisfy the Robbins-Munro sequence such that:\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha_t = \\infty $\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha^2_t < \\infty $\n",
    "\n",
    "That is why we are going to use a **decaying learning rate**, like we did in Incremental Monte Carlo that satisfies the above conditions.\n",
    "If we use a learning rate similar to the one we used in Incremental Monte Carlo, of the form $ k \\times 1/c_{\\epsilon}$ we can be sure that it satisfies the above conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Initialization\n",
    "For Q learning we keep track of the following:\n",
    " - Q value functions, initially set to $0$\n",
    " - In order to showcase how robust off policy algorithms like Q learning are, we are going to keep the epsilon and learning rate constant.\n",
    "   - `self.learning_rate` is set to $0.4$.\n",
    "   - `self.epsilon` is set to $0.1$.\n",
    " - `self.discount_factor` is set to $0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from environment import Env\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions):\n",
    "        # actions = [0, 1, 2, 3]\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.4\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q learning\n",
    "\n",
    "Q learning is an off policy algorithm that combines sampling with bootstrapping in an off policy algorithm.\n",
    "\n",
    "The update rule for Q values in Q learning is the following:\n",
    "\n",
    "$Q^{\\pi}(s_t, a_t) \\gets Q^\\pi(s_t, a_t) + \\alpha[R_t+ \\gamma max_{a’} Q^\\pi(s_t ,a’) - Q^\\pi(s_t,a_t)]$ where:\n",
    " - $Q^{\\pi}(s_t, a_t)$ - Q value of current state-action pair following the policy $\\pi$\n",
    " - $Q^{\\pi}(s_{t+1}, a_{t+1})$ - the current estimate following the policy $\\pi$ of the state value of the next state.\n",
    " - $\\alpha$ - the **learning rate**.\n",
    " Learning rate can take any value int the range $[0...1]$.\n",
    " Values closer to 0 mean that we put more value to older experiences, whereas values closer to 1 means that we put more value to latest experiences.\n",
    " In our case, the learning rate takes the value $0.4$.\n",
    " - $r_t$ - the reward at time-step $t$.\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " Traditionally used when calculating returns, now it is used when calculating **expectancies of returns**, i.e. state values.\n",
    "\n",
    "The difference $r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1}) − Q^{\\pi}(s_t, a_t)$ is commonly referred to as the **TD error**.\n",
    "\n",
    "The sum $r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1})$ is referred to as the **TD target**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    # update q function with sample <s, a, r, s'>\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        current_q = self.q_table[state][action]\n",
    "        # using Bellman Optimality Equation to update q function\n",
    "        QL_Target = reward + self.discount_factor * max(self.q_table[next_state])\n",
    "        QL_Error = QL_Target - current_q\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * QL_Error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    # get action for the state according to the q function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function table\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Main loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class QLearningAgent(QLearningAgent):\n",
    "    def mainloop(self, env, verbose=False):\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # take action and proceed one step in the environment\n",
    "                action = self.get_action(str(state))\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                # with sample <s,a,r,s'>, agent learns new q function\n",
    "                self.learn(str(state), action, reward, str(next_state))\n",
    "\n",
    "                state = next_state\n",
    "                env.print_value_all(self.q_table)\n",
    "\n",
    "                # if episode ends, then break\n",
    "                if done:\n",
    "                    if verbose:\n",
    "                        print(\"episode: \", episode,\n",
    "                              \"\\tepsilon: \", round(self.epsilon, 2),\n",
    "                              \"\\tlearning rate: \", round(self.learning_rate, 2)\n",
    "                              )\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = QLearningAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env, verbose=False)\n",
    "    except:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "Q learning does converge to an optimal policy within 60 episodes.\n",
    "\n",
    "Q learning, being an off policy algorithm, shows very robust results *even when the learning rates do not satisfy the Robbins-Munro sequence condition*.\n",
    "\n",
    "As you can see from the image below, decoupling the Q function estimator from the policy allows us *not to bootstrap penalizations in other states unnecessarily*.\n",
    "Negative Q values remain only near the triangles, whereas other Q values of other neighbouring states do not get unnecessarily negative.\n",
    "\n",
    "<h3 style=\"text-align:center\">After 100 episodes</h3>\n",
    "<img src=\"ipynb_results/q_learning_100_episodes.png\" alt=\"q_learning_100_episodes.png\" width=\"50%\" />"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}