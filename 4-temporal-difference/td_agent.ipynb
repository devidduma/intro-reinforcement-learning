{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Temporal Difference learning: TD(0)\n",
    "\n",
    "Temporal difference learning combines the bootstrapping aspect used in dynamic programming with the sampling aspect used in Monte Carlo to give us another model free policy evaluation algorithm.\n",
    "Temporal difference first analyses and changes the original update formula from incremental MC to derive an updated formula that does both bootstrapping and sampling at the same time: $ V^{\\pi}(s_t) \\gets V^{\\pi}(s_t) + \\alpha(r + \\gamma V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t))$.\n",
    "Notice the difference is instead of waiting to calculate the $G_t$ until the end of the episode, we calculate the TD target: $ r + \\gamma V^{\\pi}(s_{t+1})$ every step of the episode.\n",
    "This way, we bootstrap the information while sampling.\n",
    "Temporal Difference learning is only used in MDP settings, like most reinforcement learning algorithms.\n",
    "Nevertheless, it is a better choice than Monte Carlo methods in MDP settings with very long episodes or non-episodic domains.\n",
    "There are many versions of Temporal Difference learning, but here we are going to show the TD(0) version.\n",
    "\n",
    "##### TD methods\n",
    "\n",
    "There is actually an entire spectrum of ways we can blend Monte Carlo and dynamic programming using a method called TD(λ).\n",
    " - when $λ = 0$, we get the TD-learning formulation above, hence giving us the alias TD(0).\n",
    " - when $λ = 1$, we recover Monte Carlo policy evaluation, depending on the formulation used.\n",
    " - when $0 < λ < 1$, we get a blend of these two methods.\n",
    "\n",
    "For a more thorough treatment of TD(λ), please see Sections 7.1 and 12.1-12.5 of the book by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Temporal Difference learning:\n",
    "\n",
    "##### Model free\n",
    "Temporal Difference learning methods are model free, i.e. they do not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### Finite and discrete state and action spaces\n",
    "In order for TD(0) to work, the environment has to have a finite state and action space, because it saves state values in a dictionary internally.\n",
    "The aforementioned is only possible if the state and action spaces are finite and discrete.\n",
    "\n",
    "##### On policy\n",
    " - On policy methods attempt to evaluate or improve the policy that is used to make decisions.\n",
    " - Off policy methods evaluate or improve a policy different from that used to generate the data.\n",
    " Something very positive for off policy methods is that they can figure out the optimal policy **regardless** of the agent’s actions and motivation.\n",
    "\n",
    "TD(0) in particular is an on policy algorithm.\n",
    "\n",
    "##### Convergence\n",
    "Temporal Difference methods converge to a *local optimum* in case of infinite visits to state-action pairs.\n",
    "\n",
    "##### Bootstrapping & Sample based\n",
    "Temporal Difference learning methods combine bootstrapping with sampling.\n",
    " - The bootstrapping aspect is common with previous dynamic programming methods.\n",
    " The update formula consists in a Bellman backup over just one transition. It is executed every transition and the state value for the current state is bootstrapped from state values of sampled next states.\n",
    " The reason we are able to backup over just one transition is because we leverage the **Markovian assumption** of the domain.\n",
    " - Sampling is common with Monte Carlo methods in order to allow for a model free algorithm.\n",
    "\n",
    "##### Biased estimation of state values\n",
    "In TD learning, we bootstrap the old estimate of the sampled next state to calculate the current state's value estimate. This means that the estimate value of current state is biased by the old estimate value of the sampled next state.\n",
    "\n",
    "##### Finite & Infinite Horizon\n",
    "Temporal Difference learning methods can be used in both finite or infinite horizon settings, i.e. it works with both episodic or non-episodic domains.\n",
    "Infinite horizon settings are possible because Temporal Difference learning update rules for the state value function happen each step, not after the end of an episode like in Monte Carlo.\n",
    "\n",
    "##### Low variance\n",
    "The variance of Monte Carlo evaluation is relatively higher than TD learning because in Monte Carlo evaluation, we consider many transitions in each episode with each transition contributing variance to our estimate.\n",
    "On the other hand, TD learning only considers one transition per update, so we do not accumulate variance as quickly.\n",
    "\n",
    "##### Low data efficiency\n",
    "Monte Carlo is generally more data efficient than TD(0).\n",
    "In Monte Carlo, we update the value of a state based on the returns of the entire episode, so if there are highly positive or negative rewards\n",
    "in many trajectories in the future, these rewards will be immediately incorporated into our update\n",
    "of state values in *every state*.\n",
    "\n",
    "On the other hand in TD(0), we update the value of a state using only the\n",
    "reward in the current step and some previous estimate of the value at the next state. This means that\n",
    "if there are highly positive or negative rewards many trajectories in the future, we will only incorporate\n",
    "these into the current state's value update. This means that if a highly rewarding episode has length $L$, then\n",
    "we may need to experience that episode *$L$ times* for the information of the highly rewarding episode\n",
    "to travel all the way back to the starting state.\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Epsilon greedy policies determine how often will the agent explore and how often will the agent exploit.\n",
    "\n",
    "Furthermore, we want the epsilon greedy policy to be **greedy in the limit of exploration (GLIE)**.\n",
    " - all state-action pairs are visited an infinite number of times\n",
    " - $\\epsilon_{t} → 0$ as $ t → 0 $, i.e. the policy is greedy in the limit and converges to 0\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon \\gets 1 / ( c_{\\epsilon} \\times f_{\\epsilon})$, where $ c_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ f_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value in the range $[0...1]$.\n",
    " - setting it to $1$ means that we put as much value to future states as the current state.\n",
    " - setting it to $0$ means that we do not value future states at all, only the current state.\n",
    "\n",
    "##### Learning rate\n",
    "The learning rate *usually* takes any value in the range $[0...1]$.\n",
    " - setting a value bigger than $1$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    " - values closer to $0$ gives a higher weight to older data.\n",
    " - values closer to $1$ gives almost the same weight to old and new data.\n",
    "\n",
    "##### Theorem: Robbins-Munro sequence for Learning rate\n",
    "Finite-state and finite-action MDP's converges to the optimal action-value, i.e. Q(s, a) → q(s, a), if the following two conditions hold:\n",
    " 1. The sequence of policies $\\pi$ is GLIE\n",
    " 2. The step-sizes $\\alpha_t$ satisfy the Robbins-Munro sequence such that:\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha_t = \\infty $\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha^2_t < \\infty $\n",
    "\n",
    "That is why we are going to use a **decaying learning rate**, like we did in Incremental Monte Carlo that satisfies the above conditions.\n",
    "If we use a learning rate similar to the one we used in Incremental Monte Carlo, of the form $ k \\times 1/c_{\\epsilon}$ we can be sure that it satisfies the above conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class Tuple:\n",
    "    def __init__(self, state, action, reward, next_state, next_action, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.next_action = next_action\n",
    "        self.done = done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Temporal Difference Agent which learns from each tuple during an episode\n",
    "# render sleep time updated to 0.01\n",
    "class TDAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        self.actions = actions\n",
    "        self.discount_factor = 1\n",
    "        self.decaying_epsilon_counter = 1\n",
    "        self.decaying_epsilon_mul_factor = 0.2\n",
    "        self.epsilon = None\n",
    "        self.tuple = None\n",
    "        self.learning_rate = 1\n",
    "        self.value_table = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Tuple class\n",
    "\n",
    "We define a class Tuple that will help us save tuples of trajectories in the following fashion:\n",
    "\n",
    "$(s_t, a_t, r_t, s_{t+1}, a_{t+1}, d)$, where:\n",
    " - $s$ - current state\n",
    " - $a$ - current action\n",
    " - $r$ - reward\n",
    " - $s_{t+1}$ - next state\n",
    " - $a_{t+1}$ - next action\n",
    " - $d$ - boolean denoting wether the current Tuple is the last one in the episode.\n",
    "\n",
    "Notice that:\n",
    " - we save $s_{t+1}$ and $a_{t+1}$ in the `Tuple` class, since we need those values for bootstrapping when updating the state values of the current state.\n",
    " - `self.tuple` variable of the class `TDAgent` is not a list of tuples, it only contains the last tuple sampled for that episode.\n",
    " This is why we save $s_{t+1}$, $a_{t+1}$ and $d$ in the current tuple, otherwise there would be no possibility to bootstrap.\n",
    "\n",
    "##### Initialization of TDAgent\n",
    "\n",
    "For Temporal Difference learning we keep track of the following:\n",
    " - state value functions, initially set to $0$\n",
    " - `self.tuples` variable is not a list of tuples, it rather contains the latest sampled tuple.\n",
    " This is why we save $s_{t+1}$, $a_{t+1}$ and $d$ in the current tuple, otherwise there would be no possibility to bootstrap.\n",
    " It is initially set to null and it is updated each step.\n",
    " - `self.learning_rate` is initialized to $1$ and decays together with `self.epsilon` with the increasing number of episodes at the same rate.\n",
    " It is repeatedly set equal to epsilon at the end of each episode.\n",
    " - `self.discount_factor` is set to $1$.\n",
    " - we set `self.decaying_epsilon_mul_factor` to a value of $0.2$.\n",
    " This is done to allow the agent to explore longer.\n",
    " `self.epsilon` starts from $5$ and decreases with each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Temporal Difference learning\n",
    "\n",
    "Temporal Difference learning combines sampling with bootstrapping.\n",
    "Recall that the update rule to Incremental Monte Carlo was the following:\n",
    "\n",
    "$ V^{\\pi}(s_t) \\gets V^{\\pi}(s_t) + \\gamma [ G(s_t) - V^{\\pi}(s_t) ] $\n",
    "\n",
    "Recall that $G(s_t)$ is the return after rolling out the policy from time step t to termination starting at state st.\n",
    "Let's now replace $G(s_t)$ with a Bellman backup like in dynamic programming.\n",
    "That is, let's replace $G(s_t)$ with: $r_t + \\alpha V^{\\pi}(s_{t+1})$, where $r_t$ is a sample of the reward at the current time step and $V^{\\pi}(s_{t+1})$ is our current estimate of the value at the next state.\n",
    "Making this substitution gives us the TD-learning update:\n",
    "\n",
    "$ V^{\\pi}(s_t) \\gets V^{\\pi}(s_t) + \\alpha [r_t + \\gamma V^{\\pi}(s_{t+1}) − V^{\\pi}(s_t)] $ where:\n",
    " - $V^{\\pi}(s_t)$ - state value of current state following the policy $\\pi$\n",
    " - $V^{\\pi}(s_{t+1})$ - the current estimate following the policy $\\pi$ of the state value of the next state.\n",
    " - $\\alpha$ - the **learning rate**.\n",
    " Learning rate can take any value int the range $[0...1]$.\n",
    " Values closer to 0 mean that we put more value to older experiences, whereas values closer to 1 means that we put more value to latest experiences.\n",
    " In our case, the learning rate takes the value $0.4$.\n",
    " - $r_t$ - the reward at time-step $t$.\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " Traditionally used when calculating returns, now it is used when calculating **expectancies of returns**, i.e. state values.\n",
    "\n",
    "The difference $r_t + \\gamma V^{\\pi}(s_{t+1}) − V^{\\pi}(s_t)$ is commonly referred to as the **TD error**.\n",
    "\n",
    "The sum $r_t + \\gamma V^{\\pi}(s_{t+1})$ is referred to as the **TD target**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # for every tuple, agent updates v function of visited states\n",
    "    def update(self):\n",
    "        state_name = str(self.tuple.state)\n",
    "        next_state_name = str(self.tuple.next_state)\n",
    "\n",
    "        V = self.value_table[state_name]\n",
    "        next_V = self.value_table[next_state_name]\n",
    "        reward = self.tuple.reward\n",
    "\n",
    "        TD_Target = reward + self.discount_factor * next_V\n",
    "        TD_Error = TD_Target - V\n",
    "        V = V + self.learning_rate * TD_Error\n",
    "\n",
    "        self.value_table[state_name] = V\n",
    "\n",
    "        if self.tuple.done:\n",
    "            self.value_table[next_state_name] = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other methods\n",
    "\n",
    "##### Update Epsilon and Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # epsilon-greedy policy\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "\n",
    "    # decaying learning rate satisfying Robbins-Munro sequence\n",
    "    def update_learning_rate(self):\n",
    "        self.learning_rate = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "        if self.learning_rate > 1:\n",
    "            self.learning_rate = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # get action for the state according to the v function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        self.update_epsilon()\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the v function table\n",
    "            next_state = self.possible_next_state(state)\n",
    "            action = self.arg_max(next_state)\n",
    "        return int(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # append sample to memory(state, reward, done)\n",
    "    def save_tuple(self, tuple):\n",
    "        self.tuple = tuple\n",
    "\n",
    "    # compute arg_max if multiple candidates exit, pick one randomly\n",
    "    @staticmethod\n",
    "    def arg_max(next_state):\n",
    "        max_index_list = []\n",
    "        max_value = next_state[0]\n",
    "        for index, value in enumerate(next_state):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # get the possible next states\n",
    "    def possible_next_state(self, state):\n",
    "        col, row = state\n",
    "        next_state = [0.0] * 4\n",
    "\n",
    "        if row != 0:\n",
    "            next_state[0] = self.value_table[str([col, row - 1])]\n",
    "        else:\n",
    "            next_state[0] = self.value_table[str(state)]\n",
    "        if row != self.height - 1:\n",
    "            next_state[1] = self.value_table[str([col, row + 1])]\n",
    "        else:\n",
    "            next_state[1] = self.value_table[str(state)]\n",
    "        if col != 0:\n",
    "            next_state[2] = self.value_table[str([col - 1, row])]\n",
    "        else:\n",
    "            next_state[2] = self.value_table[str(state)]\n",
    "        if col != self.width - 1:\n",
    "            next_state[3] = self.value_table[str([col + 1, row])]\n",
    "        else:\n",
    "            next_state[3] = self.value_table[str(state)]\n",
    "\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # main loop\n",
    "    def mainloop(self, env, verbose = False):\n",
    "\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "\n",
    "            # update epsilon and get next action\n",
    "            action = self.get_action(state)\n",
    "            reward = 0\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # forward to next state. reward is number and done is boolean\n",
    "                next_state, next_reward, done = env.step(action)\n",
    "\n",
    "                # update epsilon and get next action\n",
    "                next_action = self.get_action(next_state)\n",
    "\n",
    "                # save only tuple\n",
    "                self.save_tuple(Tuple(state, action, reward, next_state, next_action, False))\n",
    "                # update v values immediately\n",
    "                self.update()\n",
    "                # clear tuple\n",
    "                self.tuple = None\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                reward = next_reward\n",
    "\n",
    "                # at the end of each episode, print episode info\n",
    "                if done:\n",
    "                    # ---- Terminal State\n",
    "                    # save only tuple\n",
    "                    self.save_tuple(Tuple(state, action, reward, state, action, True))\n",
    "                    # update v values immediately\n",
    "                    self.update()\n",
    "                    # clear tuple\n",
    "                    self.tuple = None\n",
    "                    # ----\n",
    "\n",
    "                    self.decaying_epsilon_counter = self.decaying_epsilon_counter + 1\n",
    "                    # decaying learning rate satisfying Robbins-Munro sequence\n",
    "                    self.update_learning_rate()\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"episode: \", episode,\n",
    "                              \"\\t[3, 2]: \", round(self.value_table[\"[3, 2]\"], 2),\n",
    "                              \"\\t[2, 3]:\", round(self.value_table[\"[2, 3]\"], 2),\n",
    "                              \"\\t[2, 2]:\", round(self.value_table[\"[2, 2]\"], 2),\n",
    "                              \"\\tepsilon: \", round(self.epsilon, 2),\n",
    "                              \"\\tlearning rate: \", round(self.learning_rate, 2)\n",
    "                              )\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = TDAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env, verbose=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "TD(0) of Temporal Difference learning does converge to an optimal policy within 30 episodes.\n",
    "\n",
    "Very important to making TD(0) converge to an optimal policy in Grid World is the **decaying learning rate** that satisfies the **Robbins-Munro sequence**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}