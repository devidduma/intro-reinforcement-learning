{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Temporal Difference learning: TD(0)\n",
    "\n",
    "Temporal difference learning combines the bootstrapping aspect used in dynamic programming with the sampling aspect used in Monte Carlo to give us another model free policy evaluation algorithm.\n",
    "Temporal difference first analyses and changes the original update formula from incremental MC to derive an updated formula that does both bootstrapping and sampling at the same time: $ V_\\pi(s_t)\\ \\gets\\ V_\\pi(s_t)\\ +\\ \\alpha(R\\ +\\ \\gamma V_\\pi(s_{t+1})\\ \\ -\\ V_\\pi(s_t))$.\n",
    "Notice the difference is instead of waiting to calculate the $G_t$ until the end of the episode, we calculate the TD target: $ R\\ +\\ \\gamma V_\\pi(s_{t+1})$ every step of the episode.\n",
    "This way, we bootstrap the information while sampling.\n",
    "Temporal Difference learning is only used in MDP settings, like most reinforcement learning algorithms.\n",
    "Nevertheless, it is a better choice than Monte Carlo methods in MDP settings with very long episodes or non-episodic domains.\n",
    "There are many versions of Temporal Difference learning, but here we are going to show the TD(0) version.\n",
    "\n",
    "##### TD methods\n",
    "\n",
    "There is actually an entire spectrum of ways we can blend Monte Carlo and dynamic programming using a method called TD(λ).\n",
    " - when $λ = 0$, we get the TD-learning formulation above, hence giving us the alias TD(0).\n",
    " - when $λ = 1$, we recover Monte Carlo policy evaluation, depending on the formulation used.\n",
    " - when $0 < λ < 1$, we get a blend of these two methods.\n",
    "\n",
    "For a more thorough treatment of TD(λ), please see Sections 7.1 and 12.1-12.5 of the book by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Temporal Difference learning:\n",
    "\n",
    "##### Model free\n",
    "Temporal Difference learning methods are model free, i.e. they do not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### Convergence\n",
    "Temporal Difference methods converge to a *local optimum*.\n",
    "\n",
    "##### Bootstrapping & Sample based\n",
    "Temporal Difference learning methods combine bootstrapping with sampling.\n",
    " - The bootstrapping aspect is common with previous dynamic programming methods.\n",
    " The update formula consists in a Bellman backup over just one transition. It is executed every transition and the state value for the current state is bootstrapped from state values of previous states.\n",
    " The reason we are able to backup over just one transition in dynamic programming is because we leverage the **Markovian assumption** of the domain.\n",
    " - Sampling is common with Monte Carlo methods in order to allow for a model free algorithm.\n",
    "\n",
    "##### Biased estimation of state values\n",
    "In TD learning, we bootstrap the next state's value estimate to get the current state's value estimate, so the estimate of current state is biased by the estimated value of the next state.\n",
    "\n",
    "##### Finite & Infinite Horizon\n",
    "Temporal Difference learning methods can be used in both finite or infinite horizon settings, i.e. it works with both episodic or non-episodic domains.\n",
    "Infinite horizon settings are possible because Temporal Difference learning update rules for the state value function happen each step, not after the end of an episode like in Monte Carlo.\n",
    "\n",
    "##### Low variance\n",
    "The variance of Monte Carlo evaluation is relatively higher than TD learning because in Monte Carlo evaluation, we consider many transitions in each episode with each transition contributing variance to our estimate.\n",
    "On the other hand, TD learning only considers one transition per update, so we do not accumulate variance as quickly.\n",
    "\n",
    "##### Low data efficiency\n",
    "Monte Carlo is generally more data efficient than TD(0).\n",
    "In Monte Carlo, we update the value of a state based on the returns of the entire episode, so if there are highly positive or negative rewards\n",
    "in many trajectories in the future, these rewards will be immediately incorporated into our update\n",
    "of state values in *every state*.\n",
    "\n",
    "On the other hand in TD(0), we update the value of a state using only the\n",
    "reward in the current step and some previous estimate of the value at the next state. This means that\n",
    "if there are highly positive or negative rewards many trajectories in the future, we will only incorporate\n",
    "these into the current state's value update. This means that if a highly rewarding episode has length L, then\n",
    "we may need to experience that episode *L times* for the information of the highly rewarding episode\n",
    "to travel all the way back to the starting state.\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Epsilon greedy policies are always nice to use and combining them with Temporal Difference learning methods is a good idea.\n",
    "We set an epsilon value for the policy that decays with each step, denoting the probability that the next action will be random.\n",
    "This way, we allow our agent to explore more in the beginning, where epsilon is near 1 and exploit what he has learned during the late steps, where the epsilon is near 0.\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon = 1 / ( C_{\\epsilon} * F_{\\epsilon})$, where $ C_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ F_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value in the range $[0...1]$ and in our case: `self.discount_factor = 1`.\n",
    "By setting it to $1$ we basically mean that we put as much value to future states as the current state.\n",
    "\n",
    "##### Learning rate\n",
    "The learning rate *usually* takes any value in the range $[0...1]$.\n",
    " - setting a value bigger than $1$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    " - values closer to $0$ gives a higher weight to older data.\n",
    " - values closer to $1$ gives almost the same weight to old and new data.\n",
    "\n",
    "The learning rate can also be a **decaying learning rate**, like we did in Incremental Monte Carlo.\n",
    "Because of the similarities between Incremental Monte Carlo and TD(0), we use a decaying learning rate for TD(0) too.\n",
    "It is initialized to $1$ and decays with increasing number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class Tuple:\n",
    "    def __init__(self, state, action, reward, next_state, next_action, done):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.next_action = next_action\n",
    "        self.done = done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Temporal Difference Agent which learns from each tuple during an episode\n",
    "# render sleep time updated to 0.01\n",
    "class TDAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        self.actions = actions\n",
    "        self.discount_factor = 1\n",
    "        self.decaying_epsilon_counter = 1\n",
    "        self.decaying_epsilon_mul_factor = 0.2\n",
    "        self.epsilon = None\n",
    "        self.tuple = None\n",
    "        self.learning_rate = 1\n",
    "        self.value_table = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Tuple class\n",
    "\n",
    "We define a class Tuple that will help us save tuples of trajectories in the following fashion:\n",
    "\n",
    "$(S, A, R, S_{next}, A_{next})$, where:\n",
    " - $S$ - current state\n",
    " - $A$ - current action\n",
    " - $R$ - reward\n",
    " - $S_{next}$ - next state\n",
    " - $A_{next}$ - next action\n",
    " - $D$ - boolean denoting wether the current Tuple is the last one in the episode.\n",
    "\n",
    "Notice that:\n",
    " - we save $S_{next}$ and $A_{next}$ in the `Tuple` class, since we need those values for bootstrapping when updating the state values of the current state.\n",
    " - `self.tuple` variable of the class `TDAgent` is not a list of tuples, it only contains the last tuple sampled for that episode.\n",
    " This is why we save $S_{next}$, $A_{next}$ and $D$ in the current tuple, otherwise there would be no possibility to bootstrap.\n",
    "\n",
    "##### Initialization of TDAgent\n",
    "\n",
    "For Temporal Difference learning we keep track of the following:\n",
    " - state value functions, initially set to $0$\n",
    " - `self.tuples` variable is not a list of tuples, it rather contains the latest sampled tuple.\n",
    " This is why we save $S_{next}$, $A_{next}$ and $D$ in the current tuple, otherwise there would be no possibility to bootstrap.\n",
    " It is initially set to null and it is updated each step.\n",
    " - `self.learning_rate` is initialized to $1$ and decays with the increasing number of episodes.\n",
    " - `self.discount_factor` is set to `agent.learning_rate = agent.epsilon / 5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Temporal Difference learning\n",
    "\n",
    "Temporal Difference learning combines sampling with bootstrapping.\n",
    "Recall that the update rule to Incremental Monte Carlo was the following:\n",
    "\n",
    "$ V^{\\pi}(S) = V^{\\pi}(S) + \\gamma * [ G(S) - V^{\\pi}(S) ] $\n",
    "\n",
    "Recall that $G(S)$ is the return after rolling out the policy from time step t to termination starting at state st.\n",
    "Let's now replace $G(S)$ with a Bellman backup like in dynamic programming.\n",
    "That is, let's replace $G(S)$ with: $R(S) + \\alpha * V^{\\pi}(S_{next})$, where $R(S)$ is a sample of the reward at the current time step and $V^{\\pi}(S_{next})$ is our current estimate of the value at the next state.\n",
    "Making this substitution gives us the TD-learning update:\n",
    "\n",
    "$ V^{\\pi}(S) = V^{\\pi}(S) + \\alpha [R(S) + \\gamma V^{\\pi}(S_{next}) − V^{\\pi}(S)] $ where:\n",
    " - $V^{\\pi}(S)$ - state value of current state following the policy $\\pi$\n",
    " - $V^{\\pi}(S_{next}$ - the current estimate following the policy $\\pi$ of the state value of the next state.\n",
    " - $\\alpha$ - the **learning rate**.\n",
    " Learning rate can take any value int the range $[0...1]$.\n",
    " Values closer to 0 mean that we put more value to older experiences, whereas values closer to 1 means that we put more value to latest experiences.\n",
    " In our case, the learning rate takes the value $0.4$.\n",
    " - $R(S)$ - the reward at state $S$.\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " Traditionally used when calculating returns, now it is used when calculating **expectancies of returns**, i.e. state values.\n",
    "\n",
    "The difference $R(S) + \\gamma V^{\\pi}(S_{next}) − V^{\\pi}(S)$ is commonly referred to as the **TD error**.\n",
    "\n",
    "The sum $R(S) + \\gamma V^{\\pi}(S_{next})$ is referred to as the **TD target**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # for every tuple, agent updates v function of visited states\n",
    "    def update(self):\n",
    "        state_name = str(self.tuple.state)\n",
    "        next_state_name = str(self.tuple.next_state)\n",
    "\n",
    "        V = self.value_table[state_name]\n",
    "        next_V = self.value_table[next_state_name]\n",
    "        reward = self.tuple.reward\n",
    "\n",
    "        TD_Target = reward + self.discount_factor * next_V\n",
    "        TD_Error = TD_Target - V\n",
    "        V = V + self.learning_rate * TD_Error\n",
    "\n",
    "        self.value_table[state_name] = V\n",
    "\n",
    "        if self.tuple.done:\n",
    "            self.value_table[next_state_name] = reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # get action for the state according to the v function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the v function table\n",
    "            next_state = self.possible_next_state(state)\n",
    "            action = self.arg_max(next_state)\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # append sample to memory(state, reward, done)\n",
    "    def save_tuple(self, tuple):\n",
    "        self.tuple = tuple\n",
    "\n",
    "    # compute arg_max if multiple candidates exit, pick one randomly\n",
    "    @staticmethod\n",
    "    def arg_max(next_state):\n",
    "        max_index_list = []\n",
    "        max_value = next_state[0]\n",
    "        for index, value in enumerate(next_state):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # get the possible next states\n",
    "    def possible_next_state(self, state):\n",
    "        col, row = state\n",
    "        next_state = [0.0] * 4\n",
    "\n",
    "        if row != 0:\n",
    "            next_state[0] = self.value_table[str([col, row - 1])]\n",
    "        else:\n",
    "            next_state[0] = self.value_table[str(state)]\n",
    "        if row != self.height - 1:\n",
    "            next_state[1] = self.value_table[str([col, row + 1])]\n",
    "        else:\n",
    "            next_state[1] = self.value_table[str(state)]\n",
    "        if col != 0:\n",
    "            next_state[2] = self.value_table[str([col - 1, row])]\n",
    "        else:\n",
    "            next_state[2] = self.value_table[str(state)]\n",
    "        if col != self.width - 1:\n",
    "            next_state[3] = self.value_table[str([col + 1, row])]\n",
    "        else:\n",
    "            next_state[3] = self.value_table[str(state)]\n",
    "\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDAgent(TDAgent):\n",
    "    # main loop\n",
    "    def mainloop(self, env, verbose = False):\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "            action = agent.get_action(state)\n",
    "            reward = 0\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # forward to next state. reward is number and done is boolean\n",
    "                next_state, next_reward, done = env.step(action)\n",
    "\n",
    "                # get next action\n",
    "                next_action = agent.get_action(next_state)\n",
    "\n",
    "                # save only tuple\n",
    "                agent.save_tuple(Tuple(state, action, reward, next_state, next_action, False))\n",
    "                # update v values immediately\n",
    "                agent.update()\n",
    "                # clear tuple\n",
    "                agent.tuple = None\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                reward = next_reward\n",
    "\n",
    "                # at the end of each episode, print episode info\n",
    "                if done:\n",
    "                    # ---- Terminal State\n",
    "                    # save only tuple\n",
    "                    agent.save_tuple(Tuple(state, action, reward, state, action, True))\n",
    "                    # update v values immediately\n",
    "                    agent.update()\n",
    "                    # clear tuple\n",
    "                    agent.tuple = None\n",
    "                    # ----\n",
    "\n",
    "                    agent.decaying_epsilon_counter = agent.decaying_epsilon_counter + 1\n",
    "                    # decaying learning rate\n",
    "                    agent.learning_rate = 1 / (episode + 2)\n",
    "\n",
    "                    if(verbose):\n",
    "                        print(\"episode : \", episode, \"\\t[3, 2]: \", round(agent.value_table[\"[3, 2]\"], 2),\n",
    "                              \" [2, 3]:\", round(agent.value_table[\"[2, 3]\"], 2), \" [2, 2]:\", round(agent.value_table[\"[2, 2]\"], 2),\n",
    "                              \"\\tepsilon: \", round(agent.epsilon, 2))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = TDAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "TD(0) of Temporal Difference learning does converge to a solution in less than 70 episodes.\n",
    "\n",
    "Crucial to making TD(0) find a solution in Grid World is the **decaying learning rate**, that decays with increasing number of episodes.\n",
    "This is done because in TD(0), given that the agent has found a good policy, a small penalization when exploring in the late episodes does backpropagate the information to other neighbouring value states without differentiating on the action.\n",
    "That being said, because of a small penalization the agent updates the state values around the state where the penalization happened and does everything to avoid those states, even though some of those states actually lead to a solution whenever the correct action is chosen, i.e. most of the times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}