{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep SARSA\n",
    "We have already seen [SARSA](../6-sarsa/sarsa_agent.ipynb) in the previous examples.\n",
    "SARSA is an on policy method for Temporal Difference style control that uses Q values in internal calculations.\n",
    "Deep SARSA is just an extension of SARSA that uses neural networks, i.e. a *non-linear* function as state-action value function approximator.\n",
    "This is done to scale up to decision making in really large domains of huge state spaces.\n",
    "\n",
    "In our application deep SARSA shows great empirical results."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Characteristics of deep Monte Carlo Q evaluation:\n",
    "Deep SARSA inherits all the characteristics of normal [SARSA](../6-sarsa/sarsa_agent.ipynb).\n",
    "\n",
    "##### Neural Network as value function approximator\n",
    "This can be any neural network architecture that we see fit.\n",
    "In this example, we are using a very simple Dense Neural Network architecture.\n",
    "Dense Neural Networks are simply a *stack of matrices with activation functions in between*.\n",
    "\n",
    "##### Continuous or very large state and action spaces\n",
    "Using a neural network as a value function approximator has its benefits.\n",
    "It allows us to solve problems with continuous state and action spaces.\n",
    "Such problems would be almost impossible to solve with normal Q learning, as they are very resource hungry."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Initialization\n",
    "For SARSA, we keep track of the following:\n",
    " - `self.learning_rate` is set constant to $0.001$\n",
    " - `self.epsilon` is initially set to $1.0$ and decays each step taken via the variable `self.epsilon_decay = 0.999` until a minimum of `self.epsilon_min = 0.01` is reached\n",
    " - `self.discount_factor` is set to $0.99$\n",
    "\n",
    "For the neural network we keep track of the following:\n",
    " - we keep track of a model (neural network) in `self.model = self.build_model()`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "\n",
    "# this is DeepSARSA Agent for the GridWorld\n",
    "# Utilize Neural Network as q function approximator\n",
    "class DeepSARSAgent:\n",
    "    def __init__(self):\n",
    "        self.load_model = False\n",
    "        # actions which agent can do\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 15\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.epsilon = 1.  # exploration\n",
    "        self.epsilon_decay = .999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.epsilon = 0.05\n",
    "            self.model.load_weights('./save_model/deep_sarsa.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The update rule for Q values in deep SARSA is the following:\n",
    "\n",
    "$\\hat{Q}^\\pi(s_t, a_t) \\gets^{train} r_t+ \\gamma \\hat{Q}^\\pi(s_t ,a_t)$\n",
    " - $\\hat{Q}^\\pi(s_t, a_t)$ - *predicted* Q value of current state-action pair following the policy $\\pi$\n",
    " - $\\gets^{train}$ - this means train the neural network accordingly, instead of *assign* the value\n",
    " - $\\gamma$ - the **discount factor**.\n",
    "\n",
    "First things first, the update formula is very similar to the update rule we saw in Q learning, although with the following differences:\n",
    " - instead of $Q^\\pi$ we now deal with $\\hat{Q}^\\pi$, which is an approximation, the output of a neural network.\n",
    " - after calculating the result, we do not *assign* the value to $\\hat{Q}^\\pi$, but rather *train* the neural network with *gradient descent* in order to update the weights with the latest state-action value.\n",
    "\n",
    "Moreover, this seems like a simplified version of the update rule we saw for Q learning.\n",
    "Notice that the right hand side of the formula is simply a **return**.\n",
    "We use this value to update the network and we do not take into account the temporal difference between Q values.\n",
    "\n",
    "The reason behind this is that in deep SARSA, the presence of a neural network will mimic the temporal difference formula we saw on SARSA with a *learning rate*.\n",
    "In a neural network we update the weights via *backpropagation* in *gradient descent*.\n",
    "We also specify a **learning rate** in the process.\n",
    "That is why we do not need the explicit temporal difference aspect of SARSA anymore, since a similar implicit process is provided by backpropagation when we train the neural network with new data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepSARSAgent(DeepSARSAgent):\n",
    "    def train_model(self, state, action, reward, next_state, next_action, done):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        state = np.float32(state)\n",
    "        next_state = np.float32(next_state)\n",
    "        target = self.model.predict(state)[0]\n",
    "        # like Q Learning, get maximum Q value at s'\n",
    "        # But from target model\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            target[action] = (reward + self.discount_factor *\n",
    "                              self.model.predict(next_state)[0][next_action])\n",
    "\n",
    "        target = np.reshape(target, [1, 5])\n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The model: neural network\n",
    "Neural networks are built one layer after the other.\n",
    "Our model is a dense neural network, i.e. a neural network comprised of only dense layers.\n",
    "Dense layers are simply a *set of matrices* with an *activation function* in the end."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepSARSAgent(DeepSARSAgent):\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepSARSAgent(DeepSARSAgent):\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # The agent acts randomly\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            # Predict the reward value based on the given state\n",
    "            state = np.float32(state)\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Main loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = DeepSARSAgent()\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, 15])\n",
    "\n",
    "        while not done:\n",
    "            # fresh env\n",
    "            global_step += 1\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, 15])\n",
    "            next_action = agent.get_action(next_state)\n",
    "            agent.train_model(state, action, reward, next_state, next_action,\n",
    "                              done)\n",
    "            state = next_state\n",
    "            # every time step we do training\n",
    "            score += reward\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/deep_sarsa2.png\")\n",
    "                print(\"episode:\", e, \"  score:\", score, \"global_step\",\n",
    "                      global_step, \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            agent.model.save_weights(\"./save_model/deep_sarsa2.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "<h3 style=\"text-align:center\">Results</h3>\n",
    "<img src=\"./save_graph/deep_sarsa.png\" alt=\"deep_sarsa.png\" width=\"70%\" />\n",
    "\n",
    "From the graph above we can see the following:\n",
    " - the agent shows amazing results after just 20 episodes.\n",
    " - there might be a chance that such results are not always reproducible, being that we are not using any decaying learning rate that satisfies a Robbins-Munro sequence.\n",
    " Nevertheless, we should be confident that in most cases, our deep SARSA implementation will find an optimal policy given that SARSA is a reasonably complex Temporal Difference learning algorithm.\n",
    "\n",
    "We can conclude that deep SARSA has shown amazing results in our extended Grid World environment.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}