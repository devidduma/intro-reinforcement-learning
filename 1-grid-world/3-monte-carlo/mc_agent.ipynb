{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Monte Carlo on policy evaluation\n",
    "\n",
    "Monte Carlo on policy evaluation is an important model free policy evaluation algorithm which uses the popular computational method called the Monte Carlo method.\n",
    "It is important since it is usually the first model free algorithm studied in reinforcement learning.\n",
    "Model free algorithms are the ones that do not need a full knowledge of all states and transition dynamics.\n",
    "This makes Monte Carlo on policy evaluation very important since it can be applied into a wide range of real-world scenarios.\n",
    "It is also agnostic to the Markov Decision Process setting, i.e. it can be applied into reinforcement learning problems that do not follow the MDP setting.\n",
    "It is guaranteed to converge to a global optima.\n",
    "Monte Carlo on policy evaluation can be implemented in three versions, which differ on how they calculate multiple visits of the same state given an episodic (terminating) history: first visit MC, every visit MC and incremental MC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Monte Carlo on policy evaluation:\n",
    "\n",
    "##### Model free\n",
    "Monte Carlo methods are model free, i.e. they do not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### On policy / Off policy\n",
    "On policy methods attempt to evaluate or improve the policy that is used to make decisions.\n",
    "Off policy methods evaluate or improve a policy different from that used to generate the data.\n",
    "\n",
    "The Monte Carlo methods we will see here are on policy.\n",
    "Nevertheless there are also off policy versions of Monte Carlo which we will not show here.\n",
    "\n",
    "##### Convergence\n",
    "Monte Carlo policy evaluation converges to a *global optimum* value function due to the law of large numbers.\n",
    "\n",
    "##### Sample based\n",
    "Monte Carlo methods are sample based.\n",
    "Monte Carlo samples many histories for many trajectories which frees us from using a model.\n",
    "As there is no bootstrapping and we need to calculate the return of a state until the end of an episode, one sample in the case of Monte Carlo methods is the full episode.\n",
    "This means that the update rule for the state values only happens after the current episode has been completely sampled.\n",
    "\n",
    "##### Unbiased estimation of state values\n",
    "Because we are taking an average over the true distribution of returns in Monte Carlo, we obtain an unbiased estimator of the state value at each state.\n",
    "\n",
    "##### Finite Horizon\n",
    "Monte Carlo methods can only be used in a finite horizon setting, i.e. with episodic (terminating) domains only.\n",
    "This is inherent from the fact that Monte Carlo update rule for the state value function only happens at the end of each episode, i.e. they are sample based.\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Monte Carlo methods are best used with epsilon greedy policies.\n",
    "We set an epsilon value for the policy that decays with each step, denoting the probability that the next action will be random.\n",
    "This way, we allow our agent to explore more in the beginning, where epsilon is near 1 and exploit what he has learned during the late steps, where the epsilon is near 0.\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon = 1 / ( C_{\\epsilon} * F_{\\epsilon})$, where $ C_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ F_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Markov Decision Process agnostic\n",
    "Monte Carlo methods can be applied in non-MDP settings, i.e. they are MDP agnostic.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value less than $1$ and in our case: `self.discount_factor = 0.9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Initialization\n",
    "For Monte Carlo on policy evaluation we keep track of the following:\n",
    " - state value functions, initially set to $0$\n",
    " - for internal calculations we keep track of total reward up to a specific state as well as the number of times that state was visited\n",
    " - samples array contains the latest sampled episode. It is initially set to an empty array and it is cleared after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "# Monte Carlo Agent which learns every episode from the sample\n",
    "class MCAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        self.actions = actions\n",
    "        self.discount_factor = 0.9\n",
    "        self.decaying_epsilon_counter = 1\n",
    "        self.decaying_epsilon_mul_factor = 0.2\n",
    "        self.epsilon = None\n",
    "        self.samples = []\n",
    "        self.value_table = defaultdict(VisitState)\n",
    "\n",
    "# class containing information for visited states\n",
    "class VisitState:\n",
    "    def __init__(self, total_G = 0, N = 0, V = 0):\n",
    "        self.total_G = total_G\n",
    "        self.N = N\n",
    "        self.V = V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Monte Carlo on policy evaluation\n",
    "\n",
    "Monte Carlo methods sample an episode *first* and only after that do they update the V value function.\n",
    "The class `MCAgent` is a parent class for the three versions of Monte Carlo on policy evaluation: first visit Monte Carlo, every visit Monte Carlo and incremental Monte Carlo.\n",
    "\n",
    "##### Calculating the discounted returns\n",
    "\n",
    "At the end of an episode, we start by calculating the discounted returns for each visited state.\n",
    "We implement the method `preprocess_visited_states()` that calculates the discounted future sum of rewards $G_t$ for each state.\n",
    "Notice that the calculation of $G_t$ for each visited state is a common process for any version of Monte Carlo methods.\n",
    "During the calculations, the sample is reversed since it simplifies the calculations, i.e. the discount factor can be applied more easily to the $G_t$ sums in reverse and we do not need to calculate high powers of the discount factor.\n",
    "In the end it returns the states and their discounted sums in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # for each episode calculate discounted returns and return info\n",
    "    def preprocess_visited_states(self):\n",
    "        # state name and G for each state as appeared in the episode\n",
    "        all_states = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.samples):\n",
    "            state_name = str(reward[0])\n",
    "            G = reward[1] + self.discount_factor * G\n",
    "            all_states.append([state_name, G])\n",
    "        all_states.reverse()\n",
    "\n",
    "        self.decaying_epsilon_counter = self.decaying_epsilon_counter + 1\n",
    "\n",
    "        return all_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Abstract methods\n",
    "\n",
    "We define the following two abstract methods:\n",
    " - `mc()`\n",
    " - `update_global_value_table()`\n",
    "\n",
    "These have to be implemented from the specific version of Monte Carlo method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # to be defined in children classes\n",
    "    def mc(self):\n",
    "        pass\n",
    "\n",
    "    # update visited states for first visit or every visit MC\n",
    "    def update_global_value_table(self, state_name, G_t):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### First Visit Monte Carlo\n",
    "\n",
    "First visit Monte Carlo is a Monte Carlo method that considers only the first visits to a state *in one episode*.\n",
    "Notice that we can consider multiple visits to a state, but not on the same episode.\n",
    "\n",
    "We define a child class for the First Visit Monte Carlo agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_states()` method that will give us an array of visited states and their returns.\n",
    " - we make sure to check whether a state has already been visited or not.\n",
    " If it had been visited, we do not consider that state, we do not update the V values with it.\n",
    " - in the method `update_global_value_table()` we update the V values according to textbook update formulas.\n",
    " Notice that the visited states are saved in a dictionary.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for V values in the First Visit Monte Carlo is the following:\n",
    "\n",
    "$ V^{\\pi}(S) = G_{total}(S) / N(S) $ where:\n",
    " - $ N(S) $ - the number of times the state has been visited during multiple episodes.\n",
    " Notice that although we are in the first visit case, the number of times a state has been visited can be more than 1.\n",
    " That same state could have been visited multiple times in *different episodes*.\n",
    " - $ G_{total}(S) $ - cumulative return of multiple visits to that state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_agent import MCAgent, VisitState\n",
    "from environment import Env\n",
    "\n",
    "class FVMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(FVMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, update V values of visited states\n",
    "    def mc(self):\n",
    "        all_states = super(FVMCAgent, self).preprocess_visited_states()\n",
    "        visit_state = []\n",
    "        for state in all_states:\n",
    "            if state[0] not in visit_state:\n",
    "                visit_state.append(state[0])\n",
    "                self.update_global_value_table(state[0], state[1])\n",
    "\n",
    "    # update V values of visited states for first visit or every visit MC\n",
    "    def update_global_value_table(self, state_name, G_t):\n",
    "        updated = False\n",
    "        if state_name in self.value_table:\n",
    "            state = self.value_table[state_name]\n",
    "            state.total_G = state.total_G + G_t\n",
    "            state.N = state.N + 1\n",
    "            state.V = state.total_G / state.N\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.value_table[state_name] = VisitState(total_G=G_t, N=1, V=G_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every Visit Monte Carlo\n",
    "\n",
    "Every Visit Monte Carlo is a Monte Carlo method that does not differentiate if the state has been visited multiple times or not during an episode.\n",
    "\n",
    "We define a child class for the Every Visit Monte Carlo agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_states()` method that will give us an array of visited states and their returns.\n",
    " - this time we do not check whether that state has already been visited or not. We update our V values with every state in the array.\n",
    " - in the method `update_global_value_table()` we update the V values according to textbook update formulas.\n",
    " Notice that the visited states are saved in a dictionary.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for V values in the Every Visit Monte Carlo is the following:\n",
    "\n",
    "$ V^{\\pi}(S) = G_{total}(S) / N(S) $ where:\n",
    " - $ N(S) $ - the number of times the state has been visited during multiple episodes.\n",
    " One state can be visited multiple times in the same episode or in different episodes.\n",
    " - $ G_{total}(S) $ - cumulative return of multiple visits to that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_agent import MCAgent, VisitState\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class EVMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(EVMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, update V values of visited states\n",
    "    def mc(self):\n",
    "        all_states = super(EVMCAgent, self).preprocess_visited_states()\n",
    "        for state in all_states:\n",
    "            self.update_global_value_table(state[0], state[1])\n",
    "\n",
    "    # update V values of visited states for first visit or every visit MC\n",
    "    def update_global_value_table(self, state_name, G_t):\n",
    "        updated = False\n",
    "        if state_name in self.value_table:\n",
    "            state = self.value_table[state_name]\n",
    "            state.total_G = state.total_G + G_t\n",
    "            state.N = state.N + 1\n",
    "            state.V = state.total_G / state.N\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.value_table[state_name] = VisitState(total_G=G_t, N=1, V=G_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Incremental Monte Carlo\n",
    "\n",
    "Incremental Monte Carlo is a Monte Carlo method that introduces a new update rule. It has the following key characteristics:\n",
    " - most importantly, it introduces the notion of a **learning rate**, which we will see below.\n",
    " - it can take two versions: Incremental First Visit Monte Carlo and Incremental Every Visit Monte Carlo.\n",
    " We will see the latter one, although the first one can be easily derived.\n",
    "\n",
    "We define a child class for the Incremental Monte Carlo agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_states()` method that will give us an array of visited states and their returns.\n",
    " - We do not check whether that state has already been visited or not. We update our V values with every state in the array.\n",
    " - in the method `update_global_value_table()` we update the V values according to textbook update formulas.\n",
    " Notice that the visited states are saved in a dictionary.\n",
    " - `update_global_value_table()` is different for Incremental Monte Carlo.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for V values in the Incremental Monte Carlo is the following:\n",
    "\n",
    "$ V^{\\pi}(S) = V^{\\pi}(S) + \\alpha [ G(S) - V^{\\pi}(S) ] $ where:\n",
    " - $V^{\\pi}(S)$ - state value of current state following the policy $\\pi$\n",
    " - $ \\alpha $ - it is called the **learning rate**.\n",
    " In our case, we use a **decaying, step-based learning rate** which takes the value of $ \\alpha = 0.5 * 1 / N(S) $\n",
    " - $ N(S) $ - the number of times the state has been visited during multiple episodes.\n",
    " Notice that although we are in the first visit case, the number of times a state has been visited can be more than 1.\n",
    " That same state could have been visited multiple times in *different episodes*.\n",
    " - $ G(S) $ - return until the end of the episode of current state.\n",
    "\n",
    "##### Setting the learning rate\n",
    "\n",
    "Incremental Monte Carlo can be thought of as a general case of the previous two methods.\n",
    " - setting $\\alpha = 1 / N(S)$ recovers the original Monte Carlo on policy evaluation algorithms.\n",
    " - setting $\\alpha < 1 / N(S)$ gives a higher weight to older data\n",
    " - setting $\\alpha > 1 / N(S)$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    "\n",
    "If we are in a truly Markovian domain, Every Visit Monte Carlo will be more data efficient, because we update our average return for a state every time we visit the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_agent import MCAgent, VisitState\n",
    "from environment import Env\n",
    "\n",
    "class IMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(IMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, update V values of visited states\n",
    "    def mc(self):\n",
    "        all_states = super(IMCAgent, self).preprocess_visited_states()\n",
    "        for state in all_states:\n",
    "            self.update_global_visit_state(state[0], state[1])\n",
    "\n",
    "    # redefined V value update of visited states for incremental MC\n",
    "    def update_global_visit_state(self, state_name, G_t):\n",
    "        updated = False\n",
    "        if state_name in self.value_table:\n",
    "            state = self.value_table[state_name]\n",
    "            state.N = state.N + 1\n",
    "            learning_rate = 0.5 * 1 / state.N\n",
    "            state.V = state.V + learning_rate * (G_t - state.V)\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.value_table[state_name] = VisitState(total_G=G_t, N=1, V=G_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # get action for the state according to the v function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the v function table\n",
    "            next_state = self.possible_next_state(state)\n",
    "            action = self.arg_max(next_state)\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # append sample to memory(state, reward, done)\n",
    "    def save_sample(self, state, reward, done):\n",
    "        self.samples.append([state, reward, done])\n",
    "\n",
    "    # compute arg_max if multiple candidates exit, pick one randomly\n",
    "    @staticmethod\n",
    "    def arg_max(next_state):\n",
    "        max_index_list = []\n",
    "        max_value = next_state[0]\n",
    "        for index, value in enumerate(next_state):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # get the possible next states\n",
    "    def possible_next_state(self, state):\n",
    "        col, row = state\n",
    "        next_state = [0.0] * 4\n",
    "\n",
    "        if row != 0:\n",
    "            next_state[0] = self.value_table[str([col, row - 1])].V\n",
    "        else:\n",
    "            next_state[0] = self.value_table[str(state)].V\n",
    "        if row != self.height - 1:\n",
    "            next_state[1] = self.value_table[str([col, row + 1])].V\n",
    "        else:\n",
    "            next_state[1] = self.value_table[str(state)].V\n",
    "        if col != 0:\n",
    "            next_state[2] = self.value_table[str([col - 1, row])].V\n",
    "        else:\n",
    "            next_state[2] = self.value_table[str(state)].V\n",
    "        if col != self.width - 1:\n",
    "            next_state[3] = self.value_table[str([col + 1, row])].V\n",
    "        else:\n",
    "            next_state[3] = self.value_table[str(state)].V\n",
    "\n",
    "        return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop\n",
    "\n",
    "Since all Monte Carlo methods are closely related, we define a common function called `mainloop()` in the parent class `MCAgent`.\n",
    "All children MC agents inherit this method and can execute it in their static main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # to be called in a main loop\n",
    "    def mainloop(self, env, verbose=False):\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # forward to next state. reward is number and done is boolean\n",
    "                next_state, reward, done = env.step(action)\n",
    "                self.save_sample(next_state, reward, done)\n",
    "\n",
    "                # get next action\n",
    "                action = self.get_action(next_state)\n",
    "\n",
    "                # at the end of each episode, update the v function table\n",
    "                if done:\n",
    "                    if(verbose):\n",
    "                        print(\"episode : \", episode, \"\\t[3, 2]: \", round(self.value_table[\"[3, 2]\"].V, 2),\n",
    "                              \"\\t[2, 3]:\", round(self.value_table[\"[2, 3]\"].V, 2),\n",
    "                              \"\\t[2, 2]:\", round(self.value_table[\"[2, 2]\"].V, 2),\n",
    "                              \"\\t\\tepsilon: \", round(self.epsilon, 2))\n",
    "                    self.mc()\n",
    "                    self.samples.clear()\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implementing the main functions for the three Monte Carlo agents is pretty straightforward now.\n",
    "\n",
    "##### First Visit Monte Carlo agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = FVMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Every Visit Monte Carlo agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = EVMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Incremental Monte Carlo agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = IMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "All Monte Carlo agents will find the solution usually within 70 iterations.\n",
    "The most effective agents to solve our problem seem to be the following:\n",
    " - First Visit Monte Carlo - In First Visit Monte Carlo, we discard states visited multiple times inside an episode that have high returns in their late visits.\n",
    " Basically, we only consider the first return of that state, which is of course much less (more discounted) than the returns of late visits.\n",
    " This in turn seems to encourage our agent not to waste time going back and forth in order to avoid being penalized by the triangles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}