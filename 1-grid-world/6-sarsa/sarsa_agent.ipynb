{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SARSA\n",
    "SARSA is another Temporal Difference learning algorithm that bootstraps while sampling.\n",
    "It shares the same principles with TD learning, although SARSA uses and updates Q values, not V values like in TD learning.\n",
    "The update formula is similar to TD learning: $ Q^{\\pi}(s_t, a_t) \\gets Q^{\\pi}(s_t, a_t) + \\alpha[R_t+ \\gamma Q^{\\pi}(s_{t+1}, a_{t+1}) - Q^{\\pi}(s_t, a_t)] $.\n",
    "SARSA is of course more resource hungry, since if there are $n$ V values for TD learning, there are $n\\times m$ Q values for SARSA, where $n$ is the number of states and $m$ is the number of actions.\n",
    "Nevertheless, SARSA is the better choice for more complex MDP problems, inherent in the fact that it allows for more detailed representations of transition dynamics while using Q values.\n",
    "\n",
    " SARSA gets its name from the parts of the trajectory used in the update equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of SARSA:\n",
    "\n",
    "##### Model free\n",
    "SARSA is model free, i.e. it does not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### On policy\n",
    " - On policy methods attempt to evaluate or improve the policy that is used to make decisions.\n",
    " - Off policy methods evaluate or improve a policy different from that used to generate the data.\n",
    " Something very positive for off policy methods is that they can figure out the optimal policy **regardless** of the agent’s actions and motivation.\n",
    "\n",
    "SARSA in particular is an on policy algorithm.\n",
    "\n",
    "##### Q values: state-action pair values\n",
    "SARSA evaluates and updates Q values, i.e. state-action pair values instead of state values, i.e. V values.\n",
    "This basically means that it does not bind the expected future sum of rewards to a state, but rather binds the expectancies to state-action pairs.\n",
    "\n",
    "##### Convergence\n",
    "SARSA converges to a *local optimum* in case of infinite visits to state-action pairs.\n",
    "\n",
    "##### Bootstrapping & Sample based\n",
    "SARSA combines bootstrapping with sampling, just like TD(0) also did.\n",
    " - The bootstrapping aspect is common with previous dynamic programming methods.\n",
    " The update formula consists in a Bellman backup over just one transition. It is executed every transition and the state-action value for the current state-action pair is bootstrapped from the old estimate of the sampled next state - next action pair.\n",
    " The reason we are able to backup over just one transition is because we leverage the **Markovian assumption** of the domain.\n",
    " - Sampling is common with Monte Carlo methods in order to allow for a model free algorithm.\n",
    "\n",
    "##### Finite & Infinite Horizon\n",
    "SARSA can be used in both finite or infinite horizon settings, i.e. it works with both episodic or non-episodic domains.\n",
    "Infinite horizon settings are possible because SARSA update rules for the Q value function happen each step, not after the end of an episode like in Monte Carlo.\n",
    "\n",
    "##### Lower variance and lower data efficiency than Monte Carlo\n",
    "SARSA has lower variance and lower data efficiency when compared to Monte Carlo methods.\n",
    "This is a property inherited from Temporal Difference learning.\n",
    "For more information, have a look at [Temporal Difference learning](../4-temporal-difference/td_agent.ipynb).\n",
    "\n",
    "##### Lower data efficiency when using Q values than using V values\n",
    "Agents that work with state-action pair values usually have lower data efficiency than their counterparts working with state values while exploring.\n",
    " - more memory is used to represent state-action pair values than state values\n",
    " - more data is needed to train the agent, i.e. the agent needs to spend more time interacting with the environment to learn a good policy while exploring\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Epsilon greedy policies determine how often will the agent explore and how often will the agent exploit.\n",
    "\n",
    "Furthermore, we want the epsilon greedy policy to be **greedy in the limit of exploration (GLIE)**.\n",
    " - all state-action pairs are visited an infinite number of times\n",
    " - $\\epsilon_{t} → 0$ as $ t → 0 $, i.e. the policy is greedy in the limit and converges to 0\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon \\gets 1 / ( c_{\\epsilon} \\times f_{\\epsilon})$, where $ c_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ f_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value in the range $[0...1]$.\n",
    " - setting it to $1$ means that we put as much value to future states as the current state.\n",
    " - setting it to $0$ means that we do not value future states at all, only the current state.\n",
    "\n",
    "##### Learning rate\n",
    "The learning rate *usually* takes any value in the range $[0...1]$.\n",
    " - setting a value bigger than $1$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    " - values closer to $0$ gives a higher weight to older data.\n",
    " - values closer to $1$ gives almost the same weight to old and new data.\n",
    "\n",
    "##### Theorem: Robbins-Munro sequence for Learning rate\n",
    "Finite-state and finite-action MDP's converges to the optimal action-value, i.e. Q(s, a) → q(s, a), if the following two conditions hold:\n",
    " 1. The sequence of policies $\\pi$ from is GLIE\n",
    " 2. The step-sizes $\\alpha_t$ satisfy the Robbins-Munro sequence such that:\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha_t = \\infty $\n",
    "  - $ \\sum^{\\infty}_{t=1} \\alpha^2_t < \\infty $\n",
    "\n",
    "That is why we are going to use a **decaying learning rate**, like we did in Incremental Monte Carlo that satisfies the above conditions.\n",
    "If we use a learning rate similar to the one we used in Incremental Monte Carlo, of the form $ k \\times 1/c_{\\epsilon}$ we can be sure that it satisfies the above conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n",
    "For SARSA we keep track of the following:\n",
    " - state-action value functions, initially set to $0$\n",
    " - `self.learning_rate` is initialized to $1$ and decays together with `self.epsilon` with the increasing number of episodes at the same rate.\n",
    " It is repeatedly set equal to epsilon at the end of each episode.\n",
    " - `self.discount_factor` is set to $0.9$.\n",
    " - we set `self.decaying_epsilon_mul_factor` to a value of $0.1$, whereas for TD(0) learning the value was set to $0.2$.\n",
    " This is done to allow the agent explore longer, because as we said algorithms that work with Q values are less data efficient than their V value counterparts.\n",
    " `self.epsilon` starts from $10$ and decreases with each episode.\n",
    "\n",
    "We also choose to directly call on the `self.learn()` method with the current sampled tuple instead of first saving the tuple on `self.tuples` variable and then learning.\n",
    "It is just a matter of preference.\n",
    "We pass the tuple in the form $(s, a, r, s_{t+1}, a_{t+1})$ in the `learn()` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n",
    "For SARSA we keep track of the following:\n",
    " - state-action value functions, initially set to $0$\n",
    " - We choose to directly call on the `self.learn()` method with the current sampled tuple instead of first saving the tuple on `self.tuples` variable and then learning.\n",
    " It is just a matter of preference.\n",
    " We pass the tuple in the form $(s, a, r, s_{t+1}, a_{t+1})$ in the `learn()` method.\n",
    " - `self.learning_rate` is initialized to $1$ and decays with the increasing number of episodes.\n",
    " - `self.discount_factor` is set to $0.9$.\n",
    " - we set `self.decaying_epsilon_mul_factor` to a value of $0.1$, whereas for TD(0) learning the value was set to $0.2$.\n",
    " This is done to allow the agent explore longer, because as we said algorithms that work with Q values are less data efficient than their V value counterparts.\n",
    " `self.epsilon` starts from $10$ and decreases with each episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "# SARSA agent learns every time step from the sample <s, a, r, s', a'>\n",
    "# render sleep time updated to 0.005\n",
    "class SARSAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 1\n",
    "        self.discount_factor = 0.9\n",
    "        self.decaying_epsilon_counter = 1\n",
    "        self.decaying_epsilon_mul_factor = 0.1\n",
    "        self.epsilon = None\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### SARSA\n",
    "\n",
    "SARSA combines sampling with bootstrapping in an on policy algorithm, just like TD(0).\n",
    "Nevertheless, unlike TD(0) in SARSA we evaluate and update Q values.\n",
    "\n",
    "The update rule for Q values in SARSA is the following:\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) \\gets Q^{\\pi}(s_t, a_t) + \\alpha [r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1}) − Q^{\\pi}(s_t, a_t)] $ where:\n",
    " - $Q^{\\pi}(s_t, a_t)$ - Q value of current state-action pair following the policy $\\pi$\n",
    " - $Q^{\\pi}(s_{t+1}, a_{t+1})$ - the current estimate following the policy $\\pi$ of the state value of the next state.\n",
    " - $\\alpha$ - the **learning rate**.\n",
    " Learning rate can take any value int the range $[0...1]$.\n",
    " Values closer to 0 mean that we put more value to older experiences, whereas values closer to 1 means that we put more value to latest experiences.\n",
    " In our case, the learning rate takes the value $0.4$.\n",
    " - $r_t$ - the reward at time-step $t$.\n",
    " - $\\gamma$ - the **discount factor**.\n",
    " Traditionally used when calculating returns, now it is used when calculating **expectancies of returns**, i.e. state values.\n",
    "\n",
    "The difference $r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1}) − Q^{\\pi}(s_t, a_t)$ is commonly referred to as the **TD error**.\n",
    "\n",
    "The sum $r_t + \\gamma Q^{\\pi}(s_{t+1}, a_{t+1})$ is referred to as the **TD target**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSAgent(SARSAgent):\n",
    "    # with sample <s, a, r, s', a'>, learns new q function\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        current_q = self.q_table[state][action]\n",
    "        next_state_q = self.q_table[next_state][next_action]\n",
    "        SRS_Target = reward + self.discount_factor * next_state_q\n",
    "        SRS_Error = SRS_Target - current_q\n",
    "        new_q = current_q + self.learning_rate * SRS_Error\n",
    "        self.q_table[state][action] = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Other methods\n",
    "\n",
    "##### Update Epsilon and Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSAgent(SARSAgent):\n",
    "    # epsilon-greedy policy\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "\n",
    "    # decaying learning rate satisfying Robbins-Munro sequence\n",
    "    def update_learning_rate(self):\n",
    "        self.learning_rate = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "        if self.learning_rate > 1:\n",
    "            self.learning_rate = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSAgent(SARSAgent):\n",
    "    # get action for the state according to the q function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        self.update_epsilon()\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function table\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSAgent(SARSAgent):\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SARSAgent(SARSAgent):\n",
    "    # main loop\n",
    "    def mainloop(self, env, verbose = False):\n",
    "        for episode in range(1000):\n",
    "            # reset environment and initialize state\n",
    "            state = env.reset()\n",
    "\n",
    "            # update epsilon and get action of state from agent\n",
    "            action = self.get_action(str(state))\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # take action and proceed one step in the environment\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                # update epsilon and get next action\n",
    "                next_action = self.get_action(str(next_state))\n",
    "\n",
    "                # with sample <s,a,r,s',a'>, agent learns new q function\n",
    "                self.learn(str(state), action, reward, str(next_state), next_action)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "                # print q function of all states at screen\n",
    "                env.print_value_all(self.q_table)\n",
    "\n",
    "                # if episode ends, then break\n",
    "                if done:\n",
    "                    self.decaying_epsilon_counter = self.decaying_epsilon_counter + 1\n",
    "                    # decaying learning rate satisfying Robbins-Munro sequence\n",
    "                    self.update_learning_rate()\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"episode: \", episode,\n",
    "                              \"\\tepsilon: \", round(self.epsilon, 2),\n",
    "                              \"\\tlearning rate: \", round(self.learning_rate, 2)\n",
    "                              )\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = SARSAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env, verbose=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    "SARSA does converge to an optimal policy within 60 episodes.\n",
    "\n",
    "Very important to making SARSA converge to an optimal policy in Grid World is the **decaying learning rate** that satisfies the **Robbins-Munro sequence**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}