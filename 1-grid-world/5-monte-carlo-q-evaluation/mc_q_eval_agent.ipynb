{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Monte Carlo on policy Q evaluation\n",
    "Monte Carlo Q evaluation is a special algorithm using Monte Carlo methods.\n",
    "It inherits the same principles of other [Monte Carlo on policy evaluation](../3-monte-carlo/mc_agent.ipynb) algorithms with the distinction that it uses state-action pair values, or also called Q values instead of state values, or also called V values in the update steps.\n",
    "Most importantly, for the incremental MC Q evaluation the update formula is the following:\n",
    "$Q^{\\pi}(s_t, a_t) \\gets\\ Q^{\\pi}(s_t, a_t) + \\alpha(G_t - Q^{\\pi}(s_t, a_t))$.\n",
    "MC Q evaluation would be a great choice for exhausting the solution space of non-MDP problem settings with complex transition dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Monte Carlo on policy evaluation:\n",
    "\n",
    "##### Model free\n",
    "Monte Carlo methods are model free, i.e. they do not require full knowledge of all states or transition dynamics.\n",
    "\n",
    "##### On policy / Off policy\n",
    "On policy methods attempt to evaluate or improve the policy that is used to make decisions.\n",
    "Off policy methods evaluate or improve a policy different from that used to generate the data.\n",
    "\n",
    "The Monte Carlo methods we will see here are on policy.\n",
    "Nevertheless there are also off policy versions of Monte Carlo which we will not show here.\n",
    "\n",
    "##### Q values: state-action pair values\n",
    "Monte Carlo on policy Q evaluation evaluates and updates Q values, i.e. state-action pair values insteal of state values, i.e. V values.\n",
    "This basically means that it does not bind the expected future sum of rewards to a state, but rather binds the expectancies to state-action pairs.\n",
    "\n",
    "##### Lower data efficiency\n",
    "Agents that work with state-action pair values have lower data efficiency than their counterparts working with state values.\n",
    "Differentiating between actions for an expected return does have the following drawback:\n",
    " - more memory is used to represent state-action pair values than state values\n",
    " - more data is needed to train the agent, i.e. the agent needs to spend more time interacting with the environment to learn a good policy\n",
    "\n",
    "##### Convergence\n",
    "Monte Carlo policy evaluation converges to a *global optimum* value function due to the law of large numbers.\n",
    "\n",
    "##### Sample based\n",
    "Monte Carlo methods are sample based.\n",
    "Monte Carlo samples many histories for many trajectories which frees us from using a model.\n",
    "As there is no bootstrapping and we need to calculate the return of a state until the end of an episode, one sample in the case of Monte Carlo methods is the full episode.\n",
    "This means that the update rule for the state values only happens after the current episode has been completely sampled.\n",
    "\n",
    "##### Unbiased estimation of state values\n",
    "Because we are taking an average over the true distribution of returns in Monte Carlo, we obtain an unbiased estimator of the state value at each state.\n",
    "\n",
    "##### Finite Horizon\n",
    "Monte Carlo methods can only be used in a finite horizon setting, i.e. with episodic (terminating) domains only.\n",
    "This is inherent from the fact that Monte Carlo update rule for the state value function only happens at the end of each episode, i.e. they are sample based.\n",
    "\n",
    "##### Epsilon greedy policy\n",
    "Monte Carlo methods are best used with epsilon greedy policies.\n",
    "We set an epsilon value for the policy that decays with each step, denoting the probability that the next action will be random.\n",
    "This way, we allow our agent to explore more in the beginning, where epsilon is near 1 and exploit what he has learned during the late steps, where the epsilon is near 0.\n",
    "\n",
    "In our case, the update rule after each step for our epsilon is the following:\n",
    "$ \\epsilon \\gets 1 / ( c_{\\epsilon} * f_{\\epsilon})$, where $ c_{\\epsilon} $ is a counter that increments after each episode has ended, whereas $ f_{\\epsilon} $ is a constant factor.\n",
    "\n",
    "##### Markov Decision Process agnostic\n",
    "Monte Carlo methods can be applied in non-MDP settings, i.e. they are MDP agnostic.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value less than $1$ and in our case: `self.discount_factor = 0.9`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Initialization\n",
    "For Monte Carlo on policy evaluation we keep track of the following:\n",
    " - state value functions, initially set to $0$\n",
    " - for internal calculations we keep track of total reward up to a specific state as well as the number of times that state was visited\n",
    " - samples array contains the latest sampled episode. It is initially set to an empty array and it is cleared after each episode.\n",
    " - we set `self.decaying_epsilon_mul_factor` to a value of $0.05$, whereas for normal Monte Carlo the value was set to $0.2$.\n",
    " This is done to allow the agent explore longer, because as we said algorithms that work with Q values are less data efficient than their V value counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "# Monte Carlo Agent which learns every episodes from the sample\n",
    "class MCAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.width = 5\n",
    "        self.height = 5\n",
    "        self.actions = actions\n",
    "        self.discount_factor = 0.9\n",
    "        self.decaying_epsilon_counter = 1\n",
    "        self.decaying_epsilon_mul_factor = 0.05\n",
    "        self.samples = []\n",
    "        self.q_value_table = defaultdict(VisitStateAction)\n",
    "\n",
    "class VisitStateAction:\n",
    "    def __init__(self, total_G = 0, N = 0, Q = 0):\n",
    "        self.total_G = total_G\n",
    "        self.N = N\n",
    "        self.Q = Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Monte Carlo on policy Q evaluation\n",
    "\n",
    "Monte Carlo methods sample an episode *first* and only after that do they update the Q value function.\n",
    "The class `MCAgent` is a parent class for the three versions of Monte Carlo on policy Q evaluation: first visit Monte Carlo, every visit Monte Carlo and incremental Monte Carlo.\n",
    "\n",
    "##### Calculating the discounted returns\n",
    "\n",
    "At the end of an episode, we start by calculating the discounted returns for each visited *state-action* pair.\n",
    "We implement the method `preprocess_visited_state_actions()` that calculates the discounted future sum of rewards $G_t$ for each state-action pair.\n",
    "Notice that the calculation of $G_t$ for each visited state-action pair is a common process for any version of Monte Carlo Q evaluation methods.\n",
    "During the calculations, the sample is reversed since it simplifies the calculations, i.e. the discount factor can be applied more easily to the $G_t$ sums in reverse and we do not need to calculate high powers of the discount factor.\n",
    "In the end it returns the state-action pairs and their discounted sums in the correct order.\n",
    "\n",
    "Notice that there is a new column in the array `rewards` of the method `preprocess_visited_state_actions()`.\n",
    "It is there as a placeholder for the *actions* taken, and considers actions as part of the \"identification\" for the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "\n",
    "    # for each episode, calculate discounted returns and return info\n",
    "    def preprocess_visited_states(self):\n",
    "        # state action name and G for each state as appeared in the episode\n",
    "        all_states = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.samples):\n",
    "            # reward[0] state info, *reward[1] action* info\n",
    "            state_action_name = str([reward[0], reward[1]])\n",
    "            G = reward[2] + self.discount_factor * G\n",
    "            all_states.append([state_action_name, G])\n",
    "        all_states.reverse()\n",
    "\n",
    "        self.decaying_epsilon_counter = self.decaying_epsilon_counter + 1\n",
    "\n",
    "        return all_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Abstract methods\n",
    "\n",
    "We define the following two abstract methods:\n",
    " - `mc()`\n",
    " - `update_global_value_table()`\n",
    "\n",
    "These have to be implemented from the specific version of Monte Carlo method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # to be defined in children classes\n",
    "    def mc(self):\n",
    "        pass\n",
    "\n",
    "    # update visited states for first visit or every visit MC\n",
    "    def update_global_value_table(self, state_name, G_t):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### First Visit Monte Carlo Q evaluation\n",
    "\n",
    "First visit Monte Carlo is a Monte Carlo method that considers only the first visits to a state-action pair *in one episode*.\n",
    "Notice that we can consider multiple visits to a state-action pair, but not on the same episode.\n",
    "\n",
    "We define a child class for the First Visit Monte Carlo Q evaluation agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_state_actions()` method that will give us an array of visited state-action pairs and their returns.\n",
    " - we make sure to check whether a state-action pair has already been visited or not.\n",
    " If it had been visited, we do not consider that state, we do not update the Q values with it.\n",
    " - in the method `update_global_q_value_table()` we update the Q values according to textbook update formulas.\n",
    " Notice that the visited states are saved in a dictionary.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for Q values in the First Visit Monte Carlo Q evaluation is the following:\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) \\gets G_{total}(s_t, a_t) / N(s_t, a_t) $ where:\n",
    " - $ N(s_t, a_t) $ - the number of times the state-action pair has been visited during multiple episodes.\n",
    " Notice that although we are in the first visit case, the number of times a state-action pair has been visited can be more than 1.\n",
    " That same state-action pair could have been visited multiple times in *different episodes*.\n",
    " - $ G_{total}(s_t, a_t) $ - cumulative return of multiple visits to that state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_q_eval_agent import MCAgent, VisitStateAction\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class FVMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(FVMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, agent updates q function of visited state action pairs\n",
    "    def mc(self):\n",
    "        all_state_actions = super(FVMCAgent, self).preprocess_visited_state_actions()\n",
    "        visit_state_action = []\n",
    "        for state_action in all_state_actions:\n",
    "            if state_action[0] not in visit_state_action:\n",
    "                visit_state_action.append(state_action[0])\n",
    "                self.update_global_q_value_table(state_action[0], state_action[1])\n",
    "\n",
    "    # update visited states for first visit or every visit MC\n",
    "    def update_global_q_value_table(self, state_action_name, G_t):\n",
    "        updated = False\n",
    "        if state_action_name in self.q_value_table:\n",
    "            state_action = self.q_value_table[state_action_name]\n",
    "            state_action.total_G = state_action.total_G + G_t\n",
    "            state_action.N = state_action.N + 1\n",
    "            state_action.Q = state_action.total_G / state_action.N\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.q_value_table[state_action_name] = VisitStateAction(total_G=G_t, N=1, Q=G_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every Visit Monte Carlo Q evaluation\n",
    "\n",
    "Every Visit Monte Carlo Q evaluation is a Monte Carlo method that does not differentiate if the state has been visited multiple times or not during an episode.\n",
    "\n",
    "We define a child class for the Every Visit Monte Carlo agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_state_actions()` method that will give us an array of visited state-action pairs and their returns.\n",
    " - this time we do not check whether that state-action pair has already been visited or not. We update our Q values with every state-action pair in the array.\n",
    " - in the method `update_global_q_value_table()` we update the Q values according to textbook update formulas.\n",
    "\n",
    " Notice that the visited state-action pairs are saved in a dictionary.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for Q values in the Every Visit Monte Carlo Q evaluation is the following:\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) \\gets G_{total}(s_t, a_t) / N(s_t, a_t) $ where:\n",
    " - $ N(s_t, a_t) $ - the number of times the state-action pair has been visited during multiple episodes.\n",
    " One state-action pair can be visited multiple times in the same episode or in different episodes.\n",
    " - $ G_{total}(s_t, a_t) $ - cumulative return of multiple visits to that state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_q_eval_agent import MCAgent, VisitStateAction\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class EVMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(EVMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, agent updates q function of visited state action pairs\n",
    "    def mc(self):\n",
    "        all_state_actions = super(EVMCAgent, self).preprocess_visited_state_actions()\n",
    "        for state_action in all_state_actions:\n",
    "                self.update_global_q_value_table(state_action[0], state_action[1])\n",
    "\n",
    "    # update visited states for first visit or every visit MC\n",
    "    def update_global_q_value_table(self, state_action_name, G_t):\n",
    "        updated = False\n",
    "        if state_action_name in self.q_value_table:\n",
    "            state_action = self.q_value_table[state_action_name]\n",
    "            state_action.total_G = state_action.total_G + G_t\n",
    "            state_action.N = state_action.N + 1\n",
    "            state_action.Q = state_action.total_G / state_action.N\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.q_value_table[state_action_name] = VisitStateAction(total_G=G_t, N=1, Q=G_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Incremental Monte Carlo Q evaluation\n",
    "\n",
    "Incremental Monte Carlo Q evaluation is a Monte Carlo method that introduces a new update rule. It has the following key characteristics:\n",
    " - most importantly, it introduces the notion of a **learning rate**, which we will see below.\n",
    " - it can take two versions: Incremental First Visit Monte Carlo Q evaluation and Incremental Every Visit Monte Carlo Q evaluation.\n",
    " We will see the latter one, although the first one can be easily derived.\n",
    "\n",
    "We define a child class for the Incremental Monte Carlo Q evaluation agent.\n",
    " - in the method `mc()` we first call the `preprocess_visited_state_actions()` method that will give us an array of visited state-action pairs and their returns.\n",
    " - We do not check whether that state-action pair has already been visited or not. We update our Q values with every state-action in the array.\n",
    " - in the method `update_global_q_value_table()` we update the Q values according to textbook update formulas.\n",
    " Notice that the visited state-action pairs are saved in a dictionary.\n",
    " - `update_global_q_value_table()` is different for Incremental Monte Carlo Q evaluation.\n",
    "\n",
    "##### Update rule\n",
    "\n",
    "The update rule for Q values in the Incremental Monte Carlo Q evaluation is the following:\n",
    "\n",
    "$ Q^{\\pi}(s_t, a_t) \\gets Q^{\\pi}(s_t, a_t) + \\alpha [ G(s_t, a_t) - Q^{\\pi}(s_t, a_t) ] $ where:\n",
    " - $Q^{\\pi}(s_t, a_t)$ - state value of current state following the policy $\\pi$\n",
    " - $ \\alpha $ - it is called the **learning rate**.\n",
    " In our case, we use a **decaying, step-based learning rate** which takes the value of $ \\alpha = 0.5 * 1 / N(s_t) $\n",
    " - $ N(s_t, a_t) $ - the number of times the state has been visited during multiple episodes.\n",
    " Notice that although we are in the first visit case, the number of times a state has been visited can be more than 1.\n",
    " That same state could have been visited multiple times in *different episodes*.\n",
    " - $ G(s_t, a_t) $ - return until the end of the episode of current state.\n",
    "\n",
    "##### Setting the learning rate\n",
    "\n",
    "Incremental Monte Carlo can be thought of as a general case of the previous two methods.\n",
    " - setting $\\alpha = 1 / N(s_t)$ recovers the original Monte Carlo on policy evaluation algorithms.\n",
    " - setting $\\alpha < 1 / N(s_t)$ gives a higher weight to older data\n",
    " - setting $\\alpha > 1 / N(s_t)$ gives a higher weight to newer data, which can help learning in non-stationary domains.\n",
    "\n",
    "If we are in a truly Markovian domain, Every Visit Monte Carlo will be more data efficient, because we update our average return for a state every time we visit the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mc_q_eval_agent import MCAgent, VisitStateAction\n",
    "from environment import Env\n",
    "\n",
    "\n",
    "class IMCAgent(MCAgent):\n",
    "    def __init__(self, actions):\n",
    "        super(IMCAgent, self).__init__(actions)\n",
    "\n",
    "    # for every episode, agent updates q function of visited state action pairs\n",
    "    def mc(self):\n",
    "        all_state_actions = super(IMCAgent, self).preprocess_visited_state_actions()\n",
    "        for state_action in all_state_actions:\n",
    "            self.update_global_q_value_table(state_action[0], state_action[1])\n",
    "\n",
    "    # redefined update visited states for incremental MC\n",
    "    def update_global_q_value_table(self, state_action_name, G_t):\n",
    "        updated = False\n",
    "        if state_action_name in self.q_value_table:\n",
    "            state_action = self.q_value_table[state_action_name]\n",
    "            state_action.N = state_action.N + 1\n",
    "            learning_rate = 0.5 * 1 / state_action.N\n",
    "            state_action.Q = state_action.Q + learning_rate * (G_t - state_action.Q)\n",
    "            updated = True\n",
    "        if not updated:\n",
    "            self.q_value_table[state_action_name] = VisitStateAction(total_G=G_t, N=1, Q=G_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "\n",
    "    # get action for the state according to the q function table\n",
    "    # agent pick action of epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        epsilon = 1 / (self.decaying_epsilon_counter * self.decaying_epsilon_mul_factor)\n",
    "        if np.random.rand() < epsilon:\n",
    "            # take random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # take action according to the q function table\n",
    "            q_values = self.possible_Q_values(state)\n",
    "            action = self.arg_max(q_values)\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # append sample to memory(state, reward, done)\n",
    "    def save_sample(self, state, action, reward, done):\n",
    "        self.samples.append([state, action, reward, done])\n",
    "\n",
    "    # compute arg_max if multiple candidates exit, pick one randomly\n",
    "    @staticmethod\n",
    "    def arg_max(next_state):\n",
    "        max_index_list = []\n",
    "        max_value = next_state[0]\n",
    "        for index, value in enumerate(next_state):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # get the possible next states\n",
    "    def possible_Q_values(self, state):\n",
    "        Q_values = [self.q_value_table[str([state, x])].Q for x in range(4)]\n",
    "        return Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main loop\n",
    "\n",
    "Since all Monte Carlo methods are closely related, we define a common function called `mainloop()` in the parent class `MCAgent`.\n",
    "All children MC agents inherit this method and can execute it in their static main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCAgent(MCAgent):\n",
    "    # to be called in a main loop\n",
    "    def mainloop(self, env, verbose=False):\n",
    "        for episode in range(1000):\n",
    "            state = env.reset()\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            while True:\n",
    "                env.render()\n",
    "\n",
    "                # forward to next state. reward is number and done is boolean\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                self.save_sample(state, action, reward, done)\n",
    "\n",
    "                # update state\n",
    "                state = next_state\n",
    "                # get next action\n",
    "                action = self.get_action(next_state)\n",
    "\n",
    "                # at the end of each episode, update the q function table\n",
    "                if done:\n",
    "                    self.mc()\n",
    "                    self.samples.clear()\n",
    "\n",
    "                    if verbose:\n",
    "                        print(\"episode : \", episode, \"\\tepsilon: \", self.epsilon)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implementing the main functions for the three Monte Carlo agents is pretty straightforward now.\n",
    "\n",
    "##### First Visit Monte Carlo Q evaluation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = FVMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Every Visit Monte Carlo Q evaluation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = EVMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Incremental Monte Carlo Q evaluation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = IMCAgent(actions=list(range(env.n_actions)))\n",
    "    try:\n",
    "        agent.mainloop(env)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results\n",
    "\n",
    " - Crucial to make Monte Carlo Q evaluation work is to lower `self.decaying_epsilon_mul_factor` to a value of $0.05$, whereas for normal Monte Carlo the value was set to $0.2$.\n",
    " This is done to allow the agent explore longer, because as we said algorithms that work with Q values are less data efficient than their V value counterparts.\n",
    " - All Monte Carlo agents will converge to an optimal policy usually within 300 iterations.\n",
    " - Crucial for making Incremental Monte Carlo Q evaluation find a solution in Grid World is the **decaying learning rate**, that decays with increasing number of episodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}