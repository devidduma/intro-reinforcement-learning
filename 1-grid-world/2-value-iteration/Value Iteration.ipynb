{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value iteration is similar to policy iteration in that it is applied in a Markov Decision Process setting and follows the dynamic programming paradigm, although this time over a value space by applying a contraction operator known as the Bellman backup operator in every iterative step of the algorithm.\n",
    "It builds up on the theorem that the Bellman backup operator is a contraction operator, so applying it iteratively guarantees the convergence of the value functions to a global optimum.\n",
    "Value iteration is also rarely used in practice, since it requires full knowledge of all states and transition dynamics, which are usually not given to us.\n",
    "Nevertheless, like policy iteration this algorithm is a classic, so it is worth studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of Value Iteration:\n",
    "\n",
    "##### Model based\n",
    "Value iteration is model based, i.e. it requires full knowledge of all states, as well as transition dynamics.\n",
    "\n",
    "##### Convergence Theorem\n",
    "Value function evaluation converges to a *global optimum* for both finite and infinite horizon settings.\n",
    "\n",
    "##### Bellman optimality backup operator\n",
    "The Bellman optimality backup operator is used during value iteration to calculate the expected future sum of rewards for a given state.\n",
    "The Bellman optimality operator updates the state value with the maximal state-action value.\n",
    "This is different from the Bellman expectation operator used in policy iteration, where the state values were weighted among the best actions of the policy.\n",
    "Nevertheless, in most cases they **deliver the same results**.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value less than 1 and in our case: `self.discount_factor = 0.9`\n",
    "\n",
    "##### Initialization\n",
    "For value iteration we keep track only of state value functions in a table. State values are initialized to 0 for all states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 2-d list for the value function\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        self.discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Value Iteration iteratively updates and improves state values, until the previous table of state values and the new table of state values are the same.\n",
    "Only after the state values converge can we extract the policy.\n",
    "\n",
    "Given a state $S$, it calculates the expected future sum of rewards for that state via the following formula:\n",
    "\n",
    "$V'(S) = \\max_{k=1}^n \\{R(S, A_k) + \\alpha * \\sum_{q=1}^m P(S_q | S, A_k) * V(S_{q})\\}$\n",
    "\n",
    "where:\n",
    " - $S$ - current state\n",
    " - $S_{q}$ - possible next state when action $A$ is taken in state $S$\n",
    " - $π(S, A_k)$ - probability of taking action $A_k$ in state $S$ according to policy $π$\n",
    " - $R(S, A_k)$ - reward from the environment after taking action $A_k$ in state $S$\n",
    " - $\\alpha$ - discount factor\n",
    " - $P(S_q | S, A_k)$ - transition probability to state $S_q$ when action $A_k$ is taken in state $S$\n",
    " - $V'(S)$ - updated value function given state $S$\n",
    " - $V(S_{q})$ - current value function given next state\n",
    "\n",
    "In our example though, the environment is deterministic, not stochastic so we abstain from the usage of transition probabilities $P(S_q | S, A_k)$ in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ValueIteration(ValueIteration):\n",
    "\n",
    "    # get next value function table from the current value function table\n",
    "    def value_iteration(self):\n",
    "        next_value_table = [[0.0] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = 0.0\n",
    "                continue\n",
    "            value_list = []\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value_list.append((reward + self.discount_factor * next_value))\n",
    "            # return the maximum value(it is the optimality equation!!)\n",
    "            next_value_table[state[0]][state[1]] = round(max(value_list), 2)\n",
    "        self.value_table = next_value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple global optima\n",
    "\n",
    "Notice that in the case of policy evaluation, the state value $V(S)$ is calculated over all possible actions of that state, weighted by the probability of taking that action from the policy.\n",
    "As we are going to see below, the policy is a uniform distribution over actions that reveal the maximal expected future sums of rewards, i.e. value states of next states.\n",
    "This means that the policy evaluation step ensures the bootstrapping of multiple optimal solutions if there exist many of them, i.e. alternative solutions that are also global optima do not get lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions\n",
    "\n",
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ValueIteration(ValueIteration):\n",
    "    # get action according to the current value function table\n",
    "    def get_action(self, state):\n",
    "        action_list = []\n",
    "        max_value = -99999\n",
    "\n",
    "        if state == [2, 2]:\n",
    "            return []\n",
    "\n",
    "        # calculating q values for the all actions and\n",
    "        # append the action to action list which has maximum q value\n",
    "        for action in self.env.possible_actions:\n",
    "\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = (reward + self.discount_factor * next_value)\n",
    "\n",
    "            if value > max_value:\n",
    "                action_list.clear()\n",
    "                action_list.append(action)\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                action_list.append(action)\n",
    "\n",
    "        return action_list\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inferring the policy\n",
    "\n",
    "Inferring the policy is the last step of the value iteration algorithm.\n",
    "After executing the value iteration multiple times, the policy can be inferred from the state values in the `get_action()` method.\n",
    "In each step we select the action leading to the maximal state value.\n",
    "\n",
    "##### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    value_iteration = ValueIteration(env)\n",
    "    grid_world = GraphicDisplay(value_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<h3 style=\"text-align:center\">Initially</h3>\n",
    "<img src=\"ipynb_results/initial.png\" alt=\"initial.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Midway</h3>\n",
    "<img src=\"ipynb_results/midway.png\" alt=\"midway.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Converged</h3>\n",
    "<img src=\"ipynb_results/converged.png\" alt=\"converged.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Inferring policy</h3>\n",
    "<img src=\"ipynb_results/converged_policy_wa.png\" alt=\"converged_policy_wa.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Final</h3>\n",
    "<img src=\"ipynb_results/final.png\" alt=\"final.png\" width=\"50%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}