{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy iteration is one classic algorithm from the dynamic programming paradigm for searching over a policy space in a Markov Decision Process setting.\n",
    "It builds up on a theorem of RL which states that given any stationary policy $π$, we can have a deterministic stationary policy that is no worse than the existing policy. Policy iteration implements an iterative algorithm that always improves on an existing policy until that policy converges to a *global optimum*, which is great since most reinforcement learning algorithms only converge to a local optimum.\n",
    "However, policy iteration is rarely used in practice, since it requires *full knowledge of all states and transition dynamics*, which are usually not given to us.\n",
    "Nevertheless, it is a classic algorithm that is part of every introductory literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Policy Iteration:\n",
    "\n",
    "##### Model based\n",
    "Policy iteration is model based, i.e. it requires full knowledge of all states, as well as transition dynamics.\n",
    "\n",
    "##### Convergence\n",
    "There exist a theorem which states that given any stationary policy $π$, we\n",
    "can find a deterministic stationary policy that is no worse than the existing policy. This is done by the following steps:\n",
    " - policy evaluation - applying the Bellman expectation backup operator on the state value functions of the existing policy\n",
    " - policy improvement - choosing the actions that lead to the improved value functions\n",
    "\n",
    "Repeatedly alternating between the value function evaluation step and policy improvement step guarantees the convergence of the algorithm to a *global optimum* for both finite and infinite horizon settings.\n",
    "\n",
    "##### Bellman expectation backup operator\n",
    "The Bellman expectation backup operator is used during policy evaluation to calculate the expected future sum of rewards for a given state.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value less than 1 and in our case: `self.discount_factor = 0.9`\n",
    "\n",
    "##### Initialization\n",
    "For policy iteration we keep track of both state value functions and policies in a table.\n",
    " - State values are initialized to 0 for all states.\n",
    " - Policies are initialized uniformly between actions for all states in the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 2D list for the value function\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # list of random policy (same probability of up, down, left, right)\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # setting terminal state\n",
    "        self.policy_table[2][2] = []\n",
    "        self.discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "Policy evaluation **maps states** to **state values**.\n",
    "Given a state $S$, it calculates the expected future sum of rewards for that state via the following formula:\n",
    "\n",
    "$V'(S) = \\sum_{k=1}^n π(S, A_k) * [R(S, A_k) + \\alpha * \\sum_{q=1}^m P(S_q | S, A_k) * V(S_{q})]$\n",
    "\n",
    "where:\n",
    " - $S$ - current state\n",
    " - $S_{q}$ - possible next state when action $A$ is taken in state $S$\n",
    " - $π(S, A_k)$ - probability of taking action $A_k$ in state $S$ according to policy $π$\n",
    " - $R(S, A_k)$ - reward from the environment after taking action $A_k$ in state $S$\n",
    " - $\\alpha$ - discount factor\n",
    " - $P(S_q | S, A_k)$ - transition probability to state $S_q$ when action $A_k$ is taken in state $S$\n",
    " - $V'(S)$ - updated value function given state $S$\n",
    " - $V(S_{q})$ - current value function given next state\n",
    "\n",
    "In our example though, the environment is deterministic, not stochastic so we abstain from the usage of transition probabilities $P(S_q | S, A_k)$ in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    def policy_evaluation(self):\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # keep the value function of terminal states as 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Multiple global optima\n",
    "\n",
    "Notice that in the case of policy evaluation, the state value $V(S)$ is calculated over all possible actions of that state, weighted by the probability of taking that action from the policy.\n",
    "As we are going to see below, the policy is a uniform distribution over actions that reveal the maximal expected future sums of rewards, i.e. value states of next states.\n",
    "This means that the policy evaluation step ensures the bootstrapping of multiple optimal solutions if there exist many of them, i.e. alternative solutions that are also global optima do not get lost.\n",
    "\n",
    "### Policy improvement\n",
    "\n",
    "Policy improvement is the step that comes after policy evaluation. It has the following key aspects:\n",
    " - It uses the state values to **extract** the best actions from them and update the policy.\n",
    " - As mentioned above, we allow for multiple optimal actions that lead to multiple global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            result = [0.0, 0.0, 0.0, 0.0]  # initialize the policy\n",
    "\n",
    "            # for each action, calculate: V(S) = reward + (discount factor) * (next state value function)\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # Here we allow multiple actions with same max values, in order to find many global optima\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # probability of action\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By going back and forth between one policy evaluation step and one policy improvement step, we are guaranteed to converge, hence terminate.\n",
    "\n",
    "### Other functions\n",
    "\n",
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    # get action according to the current policy\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<h3 style=\"text-align:center\">Initially</h3>\n",
    "<img src=\"ipynb_results/initial.png\" alt=\"initial.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Midway</h3>\n",
    "<img src=\"ipynb_results/midway.png\" alt=\"midway.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Converged</h3>\n",
    "<img src=\"ipynb_results/converged_wa.png\" alt=\"converged_wa.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Final</h3>\n",
    "<img src=\"ipynb_results/final.png\" alt=\"final.png\" width=\"50%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}