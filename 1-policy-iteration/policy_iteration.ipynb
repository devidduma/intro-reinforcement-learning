{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy iteration is one classic algorithm from the dynamic programming paradigm for searching over a policy space in a Markov Decision Process setting.\n",
    "It builds up on a theorem of RL which states that given any stationary policy $π$, we can have a deterministic stationary policy that is no worse than the existing policy. Policy iteration implements an iterative algorithm that always improves on an existing policy until that policy converges to a *global optimum*, which is great since most reinforcement learning algorithms only converge to a local optimum.\n",
    "However, policy iteration is rarely used in practice, since it requires *full knowledge of all states and transition dynamics*, which are usually not given to us.\n",
    "Nevertheless, it is a classic algorithm that is part of every introductory literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of Policy Iteration:\n",
    "\n",
    "##### Model based\n",
    "Policy iteration is model based, i.e. it requires full knowledge of all states, as well as transition dynamics.\n",
    "Normally, we need to feed our policy evaluation algorithm with the rewards and transition probabilities of the domain.\n",
    "Notice that in our case, when trying to solve the simplified Grid World, the transition dynamics are deterministic and we do not need to keep track of transition probabilities.\n",
    "\n",
    "##### Finite and discrete state and action spaces\n",
    "In order for policy iteration to work, the environment has to have a finite state and action space, because it saves a model of the environment internally, i.e.:\n",
    "  - it saves state values in a table internally\n",
    "  - it saves a policy in a table internally\n",
    "\n",
    "The aforementioned are only possible if the state and action spaces are finite and discrete.\n",
    "\n",
    "##### Dynamic programming\n",
    "Policy iteration can be categorized as a case of dynamic programming optimization method.\n",
    "Dynamic programming leverages *bootstrapping* to help us get value estimates with only one backup.\n",
    "The reason we are able to backup over just one transition in dynamic programming is because we leverage the *Markovian assumption* of the domain.\n",
    "\n",
    "##### Convergence\n",
    "Policy Iteration consists of the following two steps:\n",
    " - policy evaluation - applying the Bellman expectation backup operator on the state value functions of the existing policy\n",
    " - policy improvement - choosing the actions that lead to the improved value functions\n",
    "\n",
    "Repeatedly alternating between the value function evaluation step and policy improvement step guarantees the convergence of the algorithm to a *global optima* for the policy for both finite and infinite horizon settings. This is because:\n",
    " - for policy evaluation, given that Bellman backup operator is a contraction operator we can guarantee the convergence of the value function to a *global optima* given a policy.\n",
    " - for policy improvement, there exists a theorem which states that given any stationary policy $π$, we can find a deterministic stationary policy that is no worse than the existing policy.\n",
    "\n",
    "##### Bellman expectation backup operator\n",
    "The Bellman expectation backup operator is used during policy evaluation to calculate the expected future sum of rewards for a given state.\n",
    "\n",
    "##### Discount factor\n",
    "The discount factor must take a value in the range $[0...1]$.\n",
    " - setting it to $1$ means that we put as much value to future states as the current state.\n",
    " - setting it to $0$ means that we do not value future states at all, only the current state.\n",
    "\n",
    "##### Initialization\n",
    "For policy iteration we keep track of both state value functions and policies in a table.\n",
    " - State values are initialized to 0 for all states.\n",
    " - Policies are initialized uniformly between actions for all states in the beginning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 2D list for the value function\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # list of random policy (same probability of up, down, left, right)\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # setting terminal state\n",
    "        self.policy_table[2][2] = []\n",
    "        self.discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Policy Evaluation\n",
    "\n",
    "Policy evaluation **maps states** to **state values**.\n",
    "Given a state $s$, it calculates the expected future sum of rewards for that state via the following formula:\n",
    "\n",
    "$V'(s) \\gets \\sum_{k=1}^n π(s, a_k) [r(s, a_k) + \\gamma \\sum_{q=1}^m P(s_q | s, a_k) V(s_{q})]$\n",
    "\n",
    "where:\n",
    " - $s$ - current state\n",
    " - $s_{q}$ - possible next state when action $a$ is taken in state $s$\n",
    " - $π(s, a_k)$ - probability of taking action $a_k$ in state $s$ according to policy $π$\n",
    " - $r(s, a_k)$ - reward from the environment after taking action $a_k$ in state $s$\n",
    " - $\\gamma$ - discount factor\n",
    " - $P(s_q | s, a_k)$ - transition probability to state $s_q$ when action $a_k$ is taken in state $s$\n",
    " - $V'(s)$ - updated value function given state $s$\n",
    " - $V(s_{q})$ - current value function given next state\n",
    "\n",
    "In our example though, the environment is deterministic, not stochastic so we abstain from the usage of transition probabilities $P(s_q | s, a_k)$ in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    def policy_evaluation(self):\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # keep the value function of terminal states as 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Multiple global optima\n",
    "\n",
    "Notice that in the case of policy evaluation, the state value $V(s)$ is calculated over all possible actions of that state, weighted by the probability of taking that action from the policy.\n",
    "As we are going to see below, the policy is a uniform distribution over actions that reveal the maximal expected future sums of rewards, i.e. value states of next states.\n",
    "This means that the policy evaluation step ensures the bootstrapping of multiple optimal solutions if there exist many of them, i.e. alternative solutions that are also global optima do not get lost.\n",
    "\n",
    "### Policy improvement\n",
    "\n",
    "Policy improvement is the step that comes after policy evaluation. It has the following key aspects:\n",
    " - It uses the state values to **extract** the best actions from them and update the policy.\n",
    " - As mentioned above, we allow for multiple optimal actions that lead to multiple global optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            result = [0.0, 0.0, 0.0, 0.0]  # initialize the policy\n",
    "\n",
    "            # for each action, calculate: V(S) = reward + (discount factor) * (next state value function)\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # Here we allow multiple actions with same max values, in order to find many global optima\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # probability of action\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By going back and forth between one policy evaluation step and one policy improvement step, we are guaranteed to converge, hence terminate.\n",
    "\n",
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyIteration(PolicyIteration):\n",
    "    # get action according to the current policy\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<h3 style=\"text-align:center\">Initially</h3>\n",
    "<img src=\"ipynb_results/initial.png\" alt=\"initial.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Midway</h3>\n",
    "<img src=\"ipynb_results/midway.png\" alt=\"midway.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Converged</h3>\n",
    "<img src=\"ipynb_results/converged_wa.png\" alt=\"converged_wa.png\" width=\"50%\" />\n",
    "\n",
    "<h3 style=\"text-align:center\">Final</h3>\n",
    "<img src=\"ipynb_results/final.png\" alt=\"final.png\" width=\"50%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}