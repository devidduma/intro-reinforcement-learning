{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Monte Carlo Q Evaluation\n",
    "We have already seen [Monte Carlo Q Evaluation](../9-deep-monte-carlo-q-evaluation/deep_mc_q_eval_agent.ipynb) in the previous examples.\n",
    "Monte Carlo Q evaluation is a special algorithm using Monte Carlo methods.\n",
    "Deep Monte Carlo Q evaluation is just an extension of Monte Carlo Q evaluation that uses neural networks, i.e. a *non-linear* function as state-action value function approximator.\n",
    "This is done to scale up to decision making in really large domains of huge state spaces.\n",
    "Since neural networks use gradient descent to update the weights and specify a *learning rate*, the version of Monte Carlo here resembles Incremental Monte Carlo.\n",
    "\n",
    "As we are going to see here, deep MC Q evaluation shows acceptable capabilities in learning an optimal policy in the environment of extended Grid World."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characteristics of deep Monte Carlo Q evaluation:\n",
    "Deep Monte Carlo Q evaluation inherits all the characteristics of normal [Monte Carlo Q evaluation](../5-monte-carlo-q-evaluation/mc_q_eval_agent.ipynb).\n",
    "\n",
    "##### Neural Network as value function approximator\n",
    "This can be any neural network architecture that we see fit.\n",
    "In this example, we are using a very simple Dense Neural Network architecture.\n",
    "Dense Neural Networks are simply a *stack of matrices with activation functions in between*.\n",
    "\n",
    "##### Batch training\n",
    "Each episode sampled by the agent is used for training the agent.\n",
    "Gradient descent is not applied to each tuple individually: rather in batches of tuples.\n",
    "\n",
    "##### Continuous or very large state and action spaces\n",
    "Using a neural network as a value function approximator has its benefits.\n",
    "It allows us to solve problems with continuous state and action spaces.\n",
    "Such problems would be almost impossible to solve with normal Q learning, as they are very resource hungry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialization\n",
    "For the Q learning aspect we keep track of the following:\n",
    " - `self.learning_rate` is initially set to $0.1$ and decays with decaying factor `self.learning_rate_decay = 0.001` according to the formula: $lr_t = lr_0 \\times 1 / (1 + decay\\_factor \\times iteration)$\n",
    " - `self.epsilon` is set to $1.0$ and decays each step taken via the variable `self.epsilon_decay = 0.9999` until a minimum of `self.epsilon_min = 0.01` is reached\n",
    " - `self.discount_factor` is set to $0.99$\n",
    "\n",
    "For the neural network we keep track of the following:\n",
    " - we keep track of a model (neural network) in `self.model = self.build_model()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import Env\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "# this is Deep MC Q Evaluation Agent for the GridWorld\n",
    "# Utilize Neural Network as q function approximator\n",
    "class DeepMCQEvalAgent:\n",
    "    def __init__(self):\n",
    "        self.load_model = False\n",
    "        # actions which agent can do\n",
    "        self.action_space = [0, 1, 2, 3, 4]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 15\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.1\n",
    "        self.learning_rate_decay = 0.001\n",
    "\n",
    "        self.epsilon = 1.  # exploration\n",
    "        self.epsilon_decay = .9999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.build_model()\n",
    "\n",
    "        self.samples = []\n",
    "\n",
    "        if self.load_model:\n",
    "            self.epsilon = 0.05\n",
    "            self.model.load_weights('./save_model/deep_mc_q_eval.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Deep Monte Carlo Q evaluation\n",
    "\n",
    "The process for deep Monte Carlo Q evaluation is the following:\n",
    " 1. Interact with the environment and sample an episode\n",
    " 2. Calculate returns for each state-action pair\n",
    " 3. Choose whether to keep only the first visits to state-action pairs or every visit\n",
    " 4. For each state visited, predict Q values from the model for all possible actions\n",
    " 5. Update the Q value of the chosen action from the sample with the calculated return from point 1\n",
    " 6. Feed the new updated Q values to the network and do gradient descent\n",
    "\n",
    "Let us now see the code snippets that make the above recipe work:\n",
    "\n",
    "##### 2. Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    # for every episode, calculate return of visited states\n",
    "    def calculate_returns(self):\n",
    "        # state name and G for each state as appeared in the episode\n",
    "        all_states = []\n",
    "        G = 0\n",
    "        for reward in reversed(self.samples):\n",
    "            G = reward[1] + self.discount_factor * G\n",
    "            state_info = reward[0]\n",
    "            action = reward[1]\n",
    "            done = reward[3]\n",
    "            all_states.append([state_info, action, G, done])\n",
    "        all_states.reverse()\n",
    "\n",
    "        return all_states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. First-visit or Every-visit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    def first_or_every_visit_mc(self, first_visit=True):\n",
    "        all_states = self.calculate_returns()\n",
    "        visit_state_batch = []\n",
    "        action_batch = []\n",
    "        G_t_batch = []\n",
    "\n",
    "        visit_state = []\n",
    "        for state in all_states:\n",
    "            state_info = state[0]\n",
    "            action = state[1]\n",
    "            G_t = state[2]\n",
    "            done = state[3]\n",
    "            if not first_visit or str(state_info) not in visit_state:\n",
    "                visit_state.append(str(state_info))\n",
    "\n",
    "                visit_state_batch.append(state_info)\n",
    "                action_batch.append(action)\n",
    "                G_t_batch.append(G_t)\n",
    "\n",
    "        visit_state_batch = np.array(visit_state_batch, dtype=np.float32).reshape(-1, self.state_size)\n",
    "\n",
    "        #print(np.shape(visit_state_batch))\n",
    "        #print(np.shape(action_batch))\n",
    "        #print(np.shape(G_t_batch))\n",
    "        self.train_model(visit_state_batch, action_batch, G_t_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4-6. Training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    def train_model(self, visit_state_batch, action_batch, G_t_batch):\n",
    "        target_batch = self.model.predict(visit_state_batch)\n",
    "        # update target with observed G_t\n",
    "        for target, action, G_t in zip(target_batch, action_batch, G_t_batch):\n",
    "            target[action] = G_t\n",
    "\n",
    "        # make batches with target G_t (returns)\n",
    "        # and do the model fit!\n",
    "        self.model.fit(visit_state_batch, target_batch, epochs=2, verbose=0, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The model: neural network\n",
    "Neural networks are built one layer after the other.\n",
    "Our model is a dense neural network, i.e. a neural network comprised of only dense layers.\n",
    "Dense layers are simply a *set of matrices* with an *activation function* in the end.\n",
    "\n",
    "Notice that we are using the following:\n",
    " - Categorical cross-entropy as a loss function\n",
    " - Adam optimizer\n",
    " - decaying time-based learning rate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other methods\n",
    "\n",
    "##### Helper methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    # append sample to memory(state, reward, done)\n",
    "    def save_sample(self, state, action, reward, done):\n",
    "        self.samples.append([state, action, reward, done])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DeepMCQEvalAgent(DeepMCQEvalAgent):\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # The agent acts randomly\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            # Predict the reward value based on the given state\n",
    "            state = np.array(state, dtype=np.float32).reshape(-1, self.state_size)\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Main loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = DeepMCQEvalAgent()\n",
    "\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [15])\n",
    "\n",
    "        while not done:\n",
    "            if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "            # fresh env\n",
    "            global_step += 1\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [15])\n",
    "\n",
    "            # save tuple to episode\n",
    "            agent.save_sample(state, action, reward, False)\n",
    "\n",
    "            state = next_state\n",
    "            # every time step we do training\n",
    "            score += reward\n",
    "\n",
    "            state = copy.deepcopy(next_state)\n",
    "\n",
    "            if done:\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                pylab.plot(episodes, scores, 'b')\n",
    "                pylab.savefig(\"./save_graph/deep_mc_q_eval2.png\")\n",
    "\n",
    "                # last tuple\n",
    "                action = agent.get_action(state)\n",
    "                agent.save_sample(state, action, 1, True)\n",
    "\n",
    "                agent.first_or_every_visit_mc(first_visit=False)\n",
    "                agent.samples.clear()\n",
    "\n",
    "                print(\"episode:\", e,\n",
    "                      \"\\tscore:\", score,\n",
    "                      \"\\tglobal_step:\", global_step,\n",
    "                      \"\\tepsilon:\", agent.epsilon,\n",
    "                      \"\\tlearning_rate_decay:\", agent.learning_rate_decay\n",
    "                      )\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            agent.model.save_weights(\"./save_model/deep_mc_q_eval2.h5\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "<h3 style=\"text-align:center\">Results</h3>\n",
    "<img src=\"./save_graph/deep_mc_q_eval.png\" alt=\"deep_mc_q_eval.png\" width=\"70%\" />\n",
    "\n",
    "From the graph above we can see the following:\n",
    " - we can clearly see a tendency of continuously getting better at the game from the agent.\n",
    " - if we were to smooth the plot, the improvements in scores would be much more clear.\n",
    "\n",
    "We can conclude that deep Monte Carlo Q evaluation has the capability of learning and shows acceptable results in our extended Grid World environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}